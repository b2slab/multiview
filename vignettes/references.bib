@techreport{kanaan2017d,
author = {Kanaan-Izquierdo, Samir and Perera-Lluna, Alexandre},
institution = {B2SLAB - CREB - UPC},
title = {{Multiview multidimensional scaling}},
year = {2017}
}
@techreport{kanaan2017c,
author = {Kanaan-Izquierdo, Samir and Perera-Lluna, Alexandre},
institution = {B2SLAB - CREB - UPC},
title = {{Multiview t-distributed stochastic neighbour embedding}},
year = {2017}
}
@article{r2016,
abstract = {R Development Core Team (2011). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org/.},
archivePrefix = {arXiv},
arxivId = {http://www.R-project.org},
author = {{R Development Core Team}},
doi = {10.1038/sj.hdy.6800737},
eprint = {/www.R-project.org},
isbn = {3900051070},
issn = {16000706},
journal = {R Foundation for Statistical Computing Vienna Austria},
pages = {{\{}ISBN{\}} 3--900051--07--0},
pmid = {16106260},
primaryClass = {http:},
title = {{R: A Language and Environment for Statistical Computing}},
url = {http://www.r-project.org/},
volume = {0},
year = {2016}
}
@techreport{kanaan2017b,
author = {Kanaan-Izquierdo, Samir and Ziyatdinov, Andrey and Perera-Lluna, Alexandre},
institution = {B2SLAB - CREB - UPC},
title = {{Multiview: an R package for multiview pattern recognition}},
year = {2017}
}
@article{kanaan2017,
author = {Kanaan-Izquierdo, Samir and Ziyatdinov, Andrey and Perera-Lluna, Alexandre},
journal = {Pattern Recognition Letters},
title = {{Multiview and multifeature spectral clustering using common eigenvectors}},
year = {2017}
}
@inproceedings{kanaan2012,
abstract = {In this paper we propose a generic approach to the multiview clustering problem that can be applied to any number of data views and with different topologies, either continuous, discrete, graphs, or other. The proposed method is an extension of the well-established spectral clustering algorithm to integrate the information from several data views in the partition solution. The algorithm, therefore, resolves a joint cluster structure which could be present in all views, which enables researchers to better resolve data structures in data fusion problems The application of this novel clustering approach covers an extended number of machine learning unsupervised clustering problems including biomedical analysis or machine vision.},
author = {Kanaan-Izquierdo, Samir and Ziyatdinov, Andrey and Massanet, Raimon and Perera, Alexandre},
booktitle = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
doi = {10.1109/EMBC.2012.6346165},
isbn = {9781424441198},
issn = {1557170X},
pages = {1254--1257},
pmid = {23366126},
title = {{Multiview approach to spectral clustering}},
year = {2012}
}
@article{georghiades2001,
abstract = {We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render—or synthesize—images of the face under novel poses and illumination conditions. The pose space is then sampled, and for each pose the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone (based on Euclidean distance within the image space). We test our face recognition method on 4050 images from the Yale Face Database B; these images contain 405 viewing conditions (9 poses ¢ 45 illumination conditions) for 10 individuals. The method performs almost without error, except on the most extreme lighting directions, and significantly outperforms popular recognition methods that do not use a generative model.},
author = {Georghiades, Athinodoros S. and Belhumeur, Peter N. and Kriegman, David J.},
doi = {10.1109/34.927464},
isbn = {0162-8828 U6 - ctx{\_}ver=Z39.88-2004{\&}ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8{\&}rfr{\_}id=info:sid/summon.serialssolutions.com{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book{\&}rft.genre=proceeding{\&}rft.title=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence{\&}rft.atitle=From+few+to+many{\%}3A+illumination+cone+models+for+face+recognition+under+variable+lighting+and+pose{\&}rft.au=Georghiades{\%}2C+A.S{\&}rft.au=Belhumeur{\%}2C+P.N{\&}rft.au=Kriegman{\%}2C+D.J{\&}rft.date=2001-01-01{\&}rft.issn=0162-8828{\&}rft.volume=23{\&}rft.issue=6{\&}rft.spa},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Appearance-based vision,Face modeling,Face recognition,Generative models,Illumination and pose modeling,Illumination cones,Image-based rendering,Lighting},
number = {6},
pages = {643--660},
title = {{From few to many: Illumination cone models for face recognition under variable lighting and pose}},
volume = {23},
year = {2001}
}
@article{mahdavi2013,
author = {Mahdavi, Mehrdad and Yang, Tianbao and Jin, Rong},
file = {:home/samir/Dropbox/Doctorado/201609 MV tSNE/refs/mv gd/multiobjective{\_}nips{\_}2013.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1115--1123},
title = {{Stochastic Convex Optimization with Multiple Objectives}},
year = {2013}
}
@article{clemen1999,
abstract = {This paper concerns the combination of experts' probability distributions in risk analysis, discussing a variety of combination methods and attempting to highlight the important conceptual and practical issues to be considered in designing a combination process in practice. The role of experts is important because their judgments can provide valuable information, particularly in view of the limited availability of ‘‘hard data'' regarding many important uncertainties in risk analysis. Because uncertainties are represented in terms of probability distributions in probabilistic risk analysis (PRA), we consider expert information in terms of probability distributions. The motivation for the use of multiple experts is simply the desire to obtain as much information as possible. Combining experts' probability distributions summarizes the accumulated information for risk analysts and decision-makers. Procedures for combining probability distributions are often compartmentalized as mathematical aggregation methods or behavioral approaches, and we discuss both categories. However, an overall aggregation process could involve both mathematical and behavioral aspects, and no single process is best in all circumstances. An understanding of the pros and cons of different methods and the key issues to consider is valuable in the design of a combination process for a specific PRA. The output, a ‘‘combined probability distribution,'' can ideally be viewed as representing a summary of the current state of expert opinion regarding the uncertainty of interest.},
author = {Clemen, Robert T. and Winkler, Robert L.},
doi = {10.1023/A:1006917509560},
file = {:home/samir/Dropbox/Doctorado/201609 MV tSNE/refs/Opinion pooling/28.CombiningDistributions-Clemen{\&}Winkler-RA-99.pdf:pdf},
isbn = {0272-4332},
issn = {1539-6924},
journal = {Risk Analysis},
keywords = {combining probabilities,expert judgment,probability assessment},
number = {2},
pages = {155--156},
pmid = {21374516},
title = {{Combining Probability Distributiond from experts in Risk Analysis}},
volume = {19},
year = {1999}
}
@book{cooke1991,
abstract = {Chapter 3) Probabilitstic Thinking; Chapter 11) Combining Expert Opinions; Chapter 13) The Bayesian Model},
author = {Cooke, Roger M},
booktitle = {booksgooglecom},
doi = {10.1016/0040-1625(93)90030-B},
isbn = {9780195064650},
issn = {00401625},
pages = {336},
pmid = {4360994},
title = {{Experts in Uncertainty: Opinion and Subjective Probability in Science}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=4taZBr{\_}nvBgC{\&}oi=fnd{\&}pg=PP12{\&}dq=+R.+Cook,+Experts+in+Uncertainty.+Opinion+and+Subjective+Probability+in+Science,{\&}ots=k64pcm0G25{\&}sig=UW4d1vjK{\_}LU-SghkCzweVbxjAYQ},
year = {1991}
}
@article{bacharach1979,
abstract = {Several individuals have the same prior distribution of a parameter W; each makes an observation, then reveals parts of his posterior distribution. All individuals now update, taking due account of the inferences that have led the others to their announcements. The ...},
author = {Bacharach, Michael},
doi = {10.2307/2286408},
issn = {01621459},
journal = {Journal Of The American Statistical Association},
number = {368},
pages = {837--846},
title = {{Normal Bayesian dialogues}},
url = {http://links.jstor.org/sici?sici=0162-1459(197912)74:368{\%}3C837:NBD{\%}3E2.0.CO{\%}5Cn2-K{\&}origin=MSN{\%}5Cnpapers3://publication/uuid/2104E387-BBDB-48E2-88F1-BD578280AEE7},
volume = {74},
year = {1979}
}
@article{carvalho2012,
abstract = {An important question when eliciting opinions from experts is how to aggregate the reported opinions. In this paper, we propose a pooling method to aggregate expert opinions. Intuitively, it works as if the experts were continuously updating their opinions in order to accommodate the expertise of others. Each updated opinion takes the form of a linear opinion pool, where the weight that an expert assigns to a peer's opinion is inversely related to the distance between their opinions. In other words, experts are assumed to prefer opinions that are close to their own opinions. We prove that such an updating process leads to consensus, $\backslash$textit{\{}i.e.{\}}, the experts all converge towards the same opinion. Further, we show that if rational experts are rewarded using the quadratic scoring rule, then the assumption that they prefer opinions that are close to their own opinions follows naturally. We empirically demonstrate the efficacy of the proposed method using real-world data.},
archivePrefix = {arXiv},
arxivId = {1204.5399},
author = {Carvalho, Arthur and Larson, Kate},
eprint = {1204.5399},
file = {:home/samir/Dropbox/Doctorado/201609 MV tSNE/refs/Opinion pooling/1204.5399.pdf:pdf},
title = {{A Consensual Linear Opinion Pool}},
url = {http://arxiv.org/abs/1204.5399},
year = {2012}
}
@article{genest1990,
abstract = {A standard approach to the combination of probabilistic opinions involves taking a weighted linear average of the individuals distributions. This paper reviews some of the possible interpretations that have been proposed for these weights in the literature on expert use. Several paradigms for selecting weights are also considered. Special attention is devoted to the Bayesian mechanism used for updating expert weights in the face of new information. An asymptotic result is proved which highlights the importance of choosing the initial weights carefully.},
author = {Genest, Christian and McConway, Kevin J.},
doi = {10.1002/for.3980090106},
isbn = {1099-131X},
issn = {1099131X},
journal = {Journal of Forecasting},
keywords = {Combining opinions,Consensus,Expert resolution,Linear opinion pool,Weights},
number = {1},
pages = {53--73},
title = {{Allocating the weights in the linear opinion pool}},
volume = {9},
year = {1990}
}
@article{abbas2009,
abstract = {Linear and log-linear pools are widely used methods for aggregating expert belief. This paper frames the expert aggregation problem as a decision problem with scoring rules. We propose a scoring function that uses the Kullback-Leibler (KL) divergence measure between the aggregate distribution and each of the expert distributions. The asymmetric nature of the KL measure allows for a convenient scoring system for which the linear and log-linear pools provide the optimal assignment. We also propose a "goodness-of-fit" measure that determines how well each opinion pool characterizes its expert distributions, and also determines the performance of each pool under this scoring function. We work through several examples to illustrate the approach.$\backslash$n$\backslash$nKey Words: opinion pools; KL measure; relative entropy; aggregating expert opinion},
author = {Abbas, Ali E},
doi = {10.1287/deca.1080.0133},
file = {:home/samir/Dropbox/Doctorado/201609 MV tSNE/refs/Opinion pooling/abbas2009.pdf:pdf},
isbn = {1545-8490},
issn = {{\textless}null{\textgreater}},
journal = {Decision Analysis},
keywords = {2008,2009,6,accepted on november 20,after 1 revision,aggregating expert opinion,articles in advance february,history,kl measure,opinion pools,published online in,received on april 29,relative entropy},
number = {1},
pages = {25--37},
title = {{A Kullback-Leibler View of Linear and Log-Linear Pools}},
url = {papers2://publication/doi/10.1287/deca.1080.0133},
volume = {6},
year = {2009}
}
@article{bates1969,
abstract = {Aggregating information by combining forecasts from two or more forecasting methods is an alternative to using just a single method. In this paper we provide extensive empirical results showing that combined forecasts obtained through weighted averages can be quite accurate. Five procedures for estimating weights are investigated, and two appear to be superior to the others. These two procedures provide forecasts that are more accurate overall than forecasts from individual methods. Furthermore, they are superior to forecasts found from a simple unweighted average of the same methods. CR - Copyright {\&}{\#}169; 1983 Royal Statistical Society},
author = {Bates, J. M. and Granger, C. W. J.},
doi = {10.1057/jors.1969.103},
isbn = {9780511753961},
issn = {0160-5682},
journal = {Journal of the Operational Research Society},
number = {4},
pages = {451--468},
title = {{The Combination of Forecasts}},
url = {http://link.springer.com/10.1057/jors.1969.103},
volume = {20},
year = {1969}
}
@article{kyte1982,
abstract = {A computer program that progressively evaluates the hydrophilicity and hydrophobicity of a protein along its amino acid sequence has been devised. For this purpose, a hydropathy scale has been composed wherein the hydrophilic and hydrophobic properties of each of the 20 amino acid side-chains is taken into consideration. The scale is based on an amalgam of experimental observations derived from the literature. The program uses a moving-segment approach that continuously determines the average hydropathy within a segment of predetermined length as it advances through the sequence. The consecutive scores are plotted from the amino to the carboxy terminus. At the same time, a midpoint line is printed that corresponds to the grand average of the hydropathy of the amino acid compositions found in most of the sequenced proteins. In the case of soluble, globular proteins there is a remarkable correspondence between the interior portions of their sequence and the regions appearing on the hydrophobic side of the midpoint line, as well as the exterior portions and the regions on the hydrophilic side. The correlation was demonstrated by comparisons between the plotted values and known structures determined by crystallography. In the case of membrane-bound proteins, the portions of their sequences that are located within the lipid bilayer are also clearly delineated by large uninterrupted areas on the hydrophobic side of the midpoint line. As such, the membrane-spanning segments of these proteins can be identified by this procedure. Although the method is not unique and embodies principles that have long been appreciated, its simplicity and its graphic nature make it a very useful tool for the evaluation of protein structures. ?? 1982.},
author = {Kyte, Jack and Doolittle, Russell F.},
doi = {10.1016/0022-2836(82)90515-0},
isbn = {0022-2836 (Print)$\backslash$r0022-2836 (Linking)},
issn = {00222836},
journal = {Journal of Molecular Biology},
number = {1},
pages = {105--132},
pmid = {7108955},
title = {{A simple method for displaying the hydropathic character of a protein}},
volume = {157},
year = {1982}
}
@article{xue2016,
author = {Xue, Zhe and Li, Guorong and Huang, Qingming},
file = {:home/samir/Dropbox/Doctorado/201704 Tesis - refs/MV dimred/TO READ/11929-55626-1-PB.pdf:pdf},
journal = {Aaai},
keywords = {Technical Papers: Machine Learning Applications},
pages = {1366--1372},
title = {{Joint Multi-View Representation Learning and Image Tagging}},
year = {2016}
}
@article{vonahn2004,
abstract = {We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.},
author = {von Ahn, Luis and Dabbish, Laura},
doi = {10.1145/985692.985733},
isbn = {1581137028},
issn = {00201669},
journal = {Proceedings of the 2004 conference on Human factors in computing systems - CHI '04},
keywords = {World Wide Web,distributed knowledge acquisition,image labeling,online games},
pages = {319--326},
pmid = {21701501},
title = {{Labeling images with a computer game}},
url = {http://portal.acm.org/citation.cfm?doid=985692.985733},
year = {2004}
}
@article{chen2010,
author = {Chen, Xiangyu and Mu, Yadong and Yan, Shuicheng and Chua, Tat-Seng},
doi = {http://doi.acm.org/10.1145/1873951.1873959},
file = {:home/samir/Dropbox/Doctorado/201704 Tesis - refs/Datasets/Efficient{\_}large-scale{\_}image{\_}annotation{\_}by{\_}probabil.pdf:pdf},
isbn = {978-1-60558-933-6},
journal = {Proceedings of the international conference on Multimedia},
keywords = {collaborative multi-label propagation,image annotation},
number = {January 2010},
pages = {35--44},
title = {{Efficient large-scale image annotation by probabilistic collaborative multi-label propagation}},
url = {http://doi.acm.org/10.1145/1873951.1873959},
year = {2010}
}
@book{tadelis1984,
abstract = {This advanced text introduces the principles of noncooperative game theory in a direct and uncomplicated style that will acquaint students with the broad spectrum of the field while highlighting and explaining what they need to know at any given point.},
author = {Tadelis and Fudenberg, Drew and Tirole, Jean},
booktitle = {MIT Press Books},
doi = {10.4135/9781412984317},
isbn = {9780803920507},
issn = {0-262-06141-4},
pages = {xxiv+579},
pmid = {3364812506127435976},
title = {{Game Theory}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0262061414{\%}5Cnhttp://srmo.sagepub.com/view/game-theory/SAGE.xml},
volume = {2008},
year = {1984}
}
@article{fukuda2014,
author = {Fukuda, Ellen H. and Drummond, Luis Mauricio Gra{\~{n}}a},
doi = {10.1590/0101-7438.2014.034.03.0585},
file = {:home/samir/Dropbox/Doctorado/201609 MV tSNE/refs/mv gd/0101-7438-pope-34-03-0585.pdf:pdf},
issn = {0101-7438},
journal = {Pesquisa Operacional},
keywords = {method,multiobjective optimization,newton method,nonlinear optimization,projected gradient,steepest descent method},
number = {3},
pages = {585--620},
title = {{a Survey on Multiobjective Descent Methods}},
url = {http://www.scielo.br/scielo.php?script=sci{\_}arttext{\&}pid=S0101-74382014000300585{\&}lng=en{\&}nrm=iso{\&}tlng=en},
volume = {34},
year = {2014}
}
@inproceedings{ding2015,
abstract = {Multi-view data is very popular in real-world applications, as different view-points and various types of sensors help to better represent data when fused across views or modalities. Samples from different views of the same class are less similar than those with the same view but different class. We consider a more general case that prior view information of testing data is inaccessible in multi-view learning. Traditional multi-view learning algorithms were designed to obtain multiple view-specific linear projections and would fail without this prior information available. That was because they assumed the probe and gallery views were known in advance, so the correct view-specific projections were to be applied in order to better learn low-dimensional features. To address this, we propose a Low-Rank Common Subspace (LRCS) for multi-view data analysis, which seeks a common low-rank linear projection to mitigate the semantic gap among different views. The low-rank common projection is able to capture compatible intrinsic information across different views and also well-align the within-class samples from different views. Furthermore, with a low-rank constraint on the view-specific projected data and that transformed by the common subspace, the within-class samples from multiple views would concentrate together. Different from the traditional supervised multi-view algorithms, our LRCS works in a weakly supervised way, where only the view information gets observed. Such a common projection can make our model more flexible when dealing with the problem of lacking prior view information of testing data. Two scenarios of experiments, robust subspace learning and transfer learning, are conducted to evaluate our algorithm. Experimental results on several multi-view datasets reveal that our proposed method outperforms state-of-the-art, even when compared with some supervised learning methods.},
author = {Ding, Zhengming and Fu, Yun},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2014.29},
isbn = {978-1-4799-4302-9},
issn = {15504786},
keywords = {Multi-view,common subspace,low-rank},
number = {January},
pages = {110--119},
title = {{Low-Rank Common Subspace for Multi-view Learning}},
volume = {2015-Janua},
year = {2015}
}
@article{sokal1962,
abstract = {The purpose of this paper is to present a technique for comparing dendrograms resulting from numerical taxonomic research with one another and with dendrograms produced by conventional methods. One of the most frequent ways of depicting the results of studies in numerical taxonomy (Sokal, 1960; Sneath and Sokal, 1962) is by so-called dendrograms or diagrams of relationships. These are tree-like schemes which indicate the affinity of taxa to their nearest relatives (on the basis of similarity or phenetic resemblance alone, without any necessary phylogenetic implications). These diagrams resemble the customary phylogenetic trees, but are preferred for classificatory purposes; first, because phylogenetic inferences are speculative, while similarities are factual; secondly, because they are quantitative evaluations of these similarities; and thirdly, because they lack some of the other meanings often implied in phylogenetic trees (Sneath and Sokal, 1962). Such dendrograms have been published in bacteriological work (Sneath and Cowan, 1958), in studies of bees (Michener and Sokal 1957; Sokal and Michener 1958), butterflies (Ehrlich, 1961), rice (Morishima and Oka, 1960), members of the nightshade genus Solanum (Soria and Heiser, 1961) and others.},
author = {Sokal, Robert R and Rohlf, F James},
doi = {10.2307/1217208},
isbn = {00400262},
issn = {0040-0262},
journal = {Taxon},
number = {2},
pages = {33--40},
title = {{The Comparison of Dendrograms by Objective Methods}},
volume = {11},
year = {1962}
}
@misc{altschul1990,
abstract = {A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.},
author = {Altschul, S F and Gish, W and Miller, W and Myers, E W and Lipman, D J},
booktitle = {Journal of Molecular Biology},
doi = {10.1016/S0022-2836(05)80360-2},
isbn = {0022-2836},
issn = {00222836},
number = {3},
pages = {403--410},
pmid = {2231712},
title = {{Altschul et al.. 1990. Basic Local Alignment Search Tool.pdf}},
volume = {215},
year = {1990}
}
@article{dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Dalal, N and Triggs, B},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
isbn = {0-7695-2372-2},
issn = {1063-6919},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
keywords = {human-detection,local-feature,object-detection},
pages = {886--893},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467360},
volume = {1},
year = {2005}
}
@article{evgeniou2004,
abstract = {Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.},
author = {Evgeniou, Theodoros and Pontil, Massimiliano},
doi = {10.1145/1014052.1014067},
isbn = {1581138881},
journal = {International Conference on Knowledge Discovery and Data Mining},
keywords = {kernel methods,multi-task learning,regularization,support vector machines},
pages = {109},
title = {{Regularized multi-task learning}},
url = {http://portal.acm.org/citation.cfm?id=1014067},
year = {2004}
}
@article{lizhang2015,
author = {Li, Jiayi and Zhang, Hongyan and Zhang, Liangpei},
doi = {10.1109/TGRS.2015.2421638},
file = {:home/samir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Zhang - 2015 - Superpixel-Level Multitask Joint Sparse Representation for Hyperspectral Image Classification.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
number = {June},
title = {{Superpixel-Level Multitask Joint Sparse Representation for Hyperspectral Image Classification}},
year = {2015}
}
@article{shannon1949,
abstract = {A conception of information where probabilities can be assigned coherently only insofar as information content can be described determinately (in propositional form)},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, C E and Weaver, W},
doi = {10.2307/3611062},
eprint = {9411012},
isbn = {0252725484},
issn = {07246811},
journal = {The mathematical theory of communication},
number = {4},
pages = {117},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{The Mathematical Theory of Communication}},
volume = {27},
year = {1949}
}
@article{Luxburg,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. Nevertheless, on the first glance spectral clustering looks a bit mysterious, and it is not obvious to see why it works at all and what it really does. This article is a tutorial introduction to spectral clustering. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {Planck, Max and Luxburg, Ulrike Von},
doi = {10.1007/s11222-007-9033-z},
eprint = {arXiv:0711.0189v1},
isbn = {0960-3174},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {graph laplacian,spectral clustering},
number = {March},
pages = {395--416},
pmid = {19784854},
publisher = {Springer US},
title = {{A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering}},
url = {http://www.springerlink.com/index/10.1007/s11222-007-9033-z},
volume = {17},
year = {2006}
}
@article{craven1998,
abstract = {The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system.},
author = {Craven, M and DiPasquo, D and Freitag, D and McCallum, A and Mitchell, T and Nigam, K and Slattery, S},
doi = {10.1.1.56.9488},
isbn = {0-262-51098-7},
journal = {World Wide Web Internet And Web Information Systems},
keywords = {bases,information extraction,knowledge,machine learning,relational learning,text classi cation,web spider,world wide web},
number = {1},
pages = {509--516},
title = {{Learning to Extract Symbolic Knowledge from the World Wide Web}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.4309{\%}7B{\&}{\%}7Drep=rep1{\%}7B{\&}{\%}7Dtype=pdf},
year = {1998}
}
@article{maaten2010,
abstract = {The paper presents an alternative optimization technique for t-SNE that is orders of magnitude faster than the original optimization technique, and that produces results that are at least as good.},
author = {Maaten, Laurens Van Der},
journal = {Neural Information Processing Systems (NIPS) 2010 Workshop on Challenges in Data Visualization},
number = {1},
pages = {1--5},
title = {{Fast Optimization for t-SNE}},
url = {http://homepage.tudelft.nl/19j49/Publications{\%}7B{\_}{\%}7Dfiles/nips2010.pdf},
volume = {1},
year = {2010}
}
@article{kruskal1964a,
abstract = {Multidimensional scaling is the problem of representingn objects geometrically byn points, so that the interpoint distances correspond in some sense to experimental dissimilarities between objects. In just what sense distances and dissimilarities should correspond has been left rather vague in most approaches, thus leaving these approaches logically incomplete. Our fundamental hypothesis is that dissimilarities and distances are monotonically related. We define a quantitative, intuitively satisfying measure of goodness of fit to this hypothesis. Our technique of multidimensional scaling is to compute that configuration of points which optimizes the goodness of fit. A practical computer program for doing the calculations is described in a companion paper.},
author = {Kruskal, J B},
doi = {10.1007/BF02289565},
isbn = {101137305317417429},
issn = {00333123},
journal = {Psychometrika},
number = {1},
pages = {1--27},
pmid = {20501337},
title = {{Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis}},
volume = {29},
year = {1964}
}
@article{moore2001,
abstract = {K-means is the most famous clustering algorithm. In this tutorial we review just what it is that clustering is trying to achieve, and we show the detailed reason that the k-means approach is cleverly optimizing something very meaningful. Oh yes, and we'll tell you (and show you) what the k-means algorithm actually does. You'll also learn about another famous class of clusterers: hierarchical methods (much beloved in the life sciences). Phrases like "Hierarchical Agglomerative Clustering" and "Single Linkage Clustering" will be bandied about.},
author = {Moore, A},
journal = {Statistical Data Mining Tutorials},
keywords = {and users of,andrew would be,delighted if you found,e to other teachers,material useful in giving,these slides,this source,your own},
pages = {1--24},
title = {{K-means and Hierarchical Clustering}},
url = {http://www.cs.cmu.edu/afs/cs/user/awm/web/tutorials/kmeans11.pdf},
year = {2001}
}
@article{yin2015,
abstract = {Multi-view clustering, which aims to cluster datasets with multiple sources of information, has a wide range of applications in the communities of data mining and pattern recognition. Generally, it makes use of the complementary information embedded in multiple views to improve clustering performance. Recent methods usually find a low-dimensional embedding of multi-view data, but often ignore some useful prior information that can be utilized to better discover the latent group structure of multi-view data. To alleviate this problem, a novel pairwise sparse subspace representation model for multi-view clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to harness prior information to achieve a sparse representation of each high-dimensional data point with respect to other data points in the same view. The second part aims to maximize the correlation between the representations of different views. An alternating minimization method is provided as an efficient solution for the proposed multi-view clustering algorithm. A detailed theoretical analysis is also conducted to guarantee the convergence of the proposed method. Moreover, we show that the must-link and cannot-link constraints can be naturally integrated into the proposed model to obtain a link constrained multi-view clustering model. Extensive experiments on five real world datasets demonstrate that the proposed model performs better than several state-of-the-art multi-view clustering methods.},
author = {Yin, Qiyue and Wu, Shu and He, Ran and Wang, Liang},
doi = {10.1016/j.neucom.2015.01.017},
isbn = {0925-2312},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Multi-view clustering,Pairwise sparse representation,Subspace clustering},
pages = {12--21},
title = {{Multi-view clustering via pairwise sparse subspace representation}},
volume = {156},
year = {2015}
}
@article{hardoon2014,
abstract = {This paper examines asymptotic expansions of test statistics for dimensionality and additional information in canonical correlation analysis based on a sample of size N = n + 1 on two sets of variables, i.e.,??xu ; p1 ?? 1 and xv ; p2 ?? 1. These problems are related to dimension reduction. The asymptotic approximations of the statistics have been studied extensively when dimensions p1 and p2 are fixed and the sample size N tends to infinity. However, the approximations worsen as p1 and p2 increase. This paper derives asymptotic expansions of the test statistics when both the sample size and dimension are large, assuming that xu and xv have a joint (p1 + p2)-variate normal distribution. Numerical simulations revealed that this approximation is more accurate than the classical approximation as the dimension increases. ?? 2008 Elsevier Inc. All rights reserved.},
author = {Hardoon, David R and Szedmak, Sandor and Shawe-Taylor, John and Adrover, Jorge G and Donato, Stella M and Balakrishnan, N and Capitanio, A and Scarpa, B and {Di Leonardo}, Rossella and Adelfio, Giada and Bellanca, Adriana and Chiodi, Marcello and Mazzola, Salvatore and Sakurai, Tetsuro and Yamada, Tomoya and Shin, Hyejin and Lee, Seokho and Akbaş, Y and Takma, C ̧ and Analysis, Multivariate Data and Rolph, E and Tatham, Ronald L and Leoni, Renato and Analysis, Multivariate Data and Rolph, E and Tatham, Ronald L and Quarter, Spring and Henson, Robin K and Khalil, B and Ouarda, T B M J and St-Hilaire, A and Renema, W K J and Kan, Hermien E and Wieringa, B?? and Heerschap, Arend and Gonzalez, Jp and Bagnell, Ja and Cook, Simon and Oberthur, Thomas and Andersson, Steen a. and Crawford, Jesse B and Management, Rural Water and Sciences, Applied Life and Meyer, Jh and Rein, P and Turner, P and Mathias, K and Dlamini, Phesheya and Harvard, The and Neville, O and Hegde, Laxman and Vinokourov, A and Shawe-Taylor, John and Cristianini, N and Analysis, Multivariate Data and Rolph, E and Tatham, Ronald L and Verbyla, Klara and Africa, South and Verbyla, Klara and Sun, L and Ji, S and Yu, S and Ye, J and Sebatjane, Phuti and Sebatjane, Phuti and Marin, F{\'{a}}bio R and Jones, James W and Combes, Sylvie and Gonz{\'{a}}lez, Ignacio and D{\'{e}}jean, S{\'{e}}bastien and Baccini, Alain and Jehl, Nathalie and Juin, Herv{\'{e}} and Cauquil, Laurent and Gabinaud, B{\'{e}}atrice and Lebas, Fran{\c{c}}ois and Larzul, Catherine and Coudun, Christophe and G{\'{e}}gout, Jean Claude and Piedallu, Christian and Rameau, Jean Claude and Gore, R C and Writing, Scientific and Sciences, Health and Paananen, Jussi and Douaik, Ahmed and {Van Meirvenne}, Marc and T{\'{o}}th, Tibor and Veritas, Det Norske and Marketing, Directorate and Bristow, Keith L and Keating, Brian A and Mathematica, The},
doi = {10.1201/b15605-18},
isbn = {978-1-4398-9021-9},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {62H15,62H20,Additional information,Age at sexual maturity,Artificial neural networks,Asymptotic expansion,Asymptotic expansion of the distribution,Augusta Bay,Body weight,Box-core Sediment,Canonical Correlation Analysis,Canonical correlation,Canonical correlation analysis,Canonical covariate,Canonical form,Canonical vector,Climatic variables,Correlation Analysis,EcoPlant,Economics,Egg production,Egg weight,Eigenvalue,Eigenvector,Group symmetry model,High-dimensional framework,High-energy phosphoryl compound,Jackknife,Logistic regression,MR spectroscopy,Machine Learning,Maximal invariant,Metabolism,Missing value,Modelling,Monotone incomplete data,Multivariate skewness,Offshore,Order statistics,Perturbation method,Pre-industrial Level,Rabbit meat quality,Regional estimation,Scenario Analysis,Sensory attributes,Skew-normal distribution,Soil nutritional variables,Spatial autocorrelation,Species distribution modelling,Sugarcane Production,Systems Analysis,Tests for dimensionality,Trace Elements,Transgenic mice,Ungauged site,Water quality,an increasing number of,biplot,canonical correlation analysis,canonical correspondence analysis,different purposes,economics,estimation uncertainty,functional data analysis,functions were developed for,ical and chemical process,in recent years,kriging,linear mixed-effects model,modelling,models and pedotransfer,orthonormalized partial least squares,pH,prediction,primary,regularization,sample,scenario analysis,scores,secondary,singular value decomposition,soil phys-,soil texture fractions,spatial variability,species scores,species-environmental correlations,sugarcane production,systems analysis},
number = {1},
pages = {1--6},
pmid = {15516276},
title = {{Canonical Correlation Analysis}},
url = {http://exchange.dnv.com/OGPI/OffshorePubs/Members/rp-c207.pdf{\%}7B{\%}25{\%}7D5Cnhttp://www.scielo.br/scielo.php?pid=S0103-90162014000100001{\%}7B{\&}{\%}7Dscript=sci{\%}7B{\_}{\%}7Darttext{\%}7B{\%}25{\%}7D5Cnpapers://23322e7b-a0cc-4788-a6e0-544263a1c348/Paper/p16500{\%}7B{\%}25{\%}7D5Cnhttp://dis},
volume = {25},
year = {2014}
}
@article{johnson1967,
abstract = {Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally connected, while the other forms clusters that are optimally compact.},
author = {Johnson, Stephen C},
doi = {10.1007/BF02289588},
isbn = {0033-3123},
issn = {00333123},
journal = {Psychometrika},
number = {3},
pages = {241--254},
pmid = {5234703},
title = {{Hierarchical clustering schemes}},
volume = {32},
year = {1967}
}
@article{maaten2014,
abstract = {The paper investigates the acceleration of t-SNE--an embedding technique that is commonly used for the visualization of high- dimensional data in scatter plots--using two tree-based algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in O(NlogN). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {Maaten, Laurens Van Der},
doi = {10.1007/s10479-011-0841-3},
eprint = {1307.1662},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {barnes-hut algorithm,dual-tree algorithm,embedding,multidimensional scaling,space-partitioning trees,t-sne},
pages = {3221--3245},
pmid = {20652508},
title = {{{\{}{\textless}{\}}Van Der Maaten - 2014 - Accelerating t-{\{}{\{}{\}}SNE{\{}{\}}{\}} using tree-based algorithms.pdf{\{}{\textgreater}{\}}}},
url = {http://jmlr.org/papers/v15/vandermaaten14a.html{\%}257B{\%}2525{\%}257D5Cnfiles/1017/JMLR-van{\%}257B{\_}{\%}257Dder{\%}257B{\_}{\%}257DMaaten-2014-Accelerating{\%}257B{\_}{\%}257Dt-SNE{\%}257B{\_}{\%}257Dusing{\%}257B{\_}{\%}257DTree-Based{\%}257B{\_}{\%}257DAlgorithms.pdf{\%}257B{\%}2525{\%}257D5Cnfiles/1019/JMLR-van{\%}257B{\_}{\%}257Dder{\%}257B{\_}{\%}257DMaaten-2014-Accelerating{\%}257},
volume = {15},
year = {2014}
}
@article{sonnhammer1998,
abstract = {Pfam contains multiple alignments and hidden Markov model based profiles (HMM-profiles) of complete protein domains. The definition of domain boundaries, family members and alignment is done semi-automatically based on expert knowledge, sequence similarity, other protein family databases and the ability of HMM-profiles to correctly identify and align the members. Release 2.0 of Pfam contains 527 manually verified families which are available for browsing and on-line searching via the World Wide Web in the UK at http://www.sanger.ac.uk/Pfam/ and in the US at http://genome.wustl. edu/Pfam/ Pfam 2.0 matches one or more domains in 50{\{}{\%}{\}} of Swissprot-34 sequences, and 25{\{}{\%}{\}} of a large sample of predicted proteins from the Caenorhabditis elegans genome.},
author = {Sonnhammer, Erik L L and Eddy, Sean R and Birney, Ewan and Bateman, Alex and Durbin, Richard},
doi = {10.1093/nar/26.1.320},
isbn = {0305-1048 (Print)},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {1},
pages = {320--322},
pmid = {9399864},
title = {{Pfam: Multiple sequence alignments and HMM-profiles of protein domains}},
volume = {26},
year = {1998}
}
@article{geng2012,
abstract = {We propose an automatic approximation of the intrinsic manifold for general semi-supervised learning problems. Unfortunately, it is not trivial to define an optimization function to obtain optimal hyperparameters. Usually, pure cross-validation is considered but it does not necessarily scale up. A second problem derives from the suboptimality incurred by discrete grid search and overfitting problems. As a consequence, we developed an ensemble manifold regularization (EMR) framework to approximate the intrinsic manifold by combining several initial guesses. Algorithmically, we designed EMR very carefully so that it (a) learns both the composite manifold and the semi-supervised classifier jointly; (b) is fully automatic for learning the intrinsic manifold hyperparameters implicitly; (c) is conditionally optimal for intrinsic manifold approximation under a mild and reasonable assumption; and (d) is scalable for a large number of candidate manifold hyperparameters, from both time and space perspectives. Extensive experiments over both synthetic and real datasets show the effectiveness of the proposed framework.},
author = {Geng, Bo and Tao, Dacheng and Xu, Chao and Yang, Linjun and Hua, Xian Sheng},
doi = {10.1109/TPAMI.2012.57},
isbn = {9781424439935},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Ensemble manifold regularization,Manifold learning,Semi-supervised learning},
number = {6},
pages = {1227--1233},
pmid = {22371429},
title = {{Ensemble manifold regularization}},
volume = {34},
year = {2012}
}
@article{hansson2005,
abstract = {This text is a non-technical overview of modern decision theory. It is intended for university students with no previous acquaintance with the subject, and was primarily written for the participants of a course on risk analysis at Uppsala University in 1994.},
archivePrefix = {arXiv},
arxivId = {http://elibrary.kiu.ac.ug:8080/jspui/handle/1/1620},
author = {Hansson, Sven Ove},
doi = {http://www.infra.kth.se/~soh/decisiontheory.pdf},
eprint = {/elibrary.kiu.ac.ug:8080/jspui/handle/1/1620},
issn = {{\{}{\textless}{\}}null{\{}{\textgreater}{\}}},
journal = {Technology},
number = {1},
pages = {1--94},
primaryClass = {http:},
title = {{Decision Theory}},
url = {http://www.mendeley.com/research/decision-theory-a-brief-introduction/},
volume = {19},
year = {2005}
}
@article{lee1999,
abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Lee, D D and Seung, H S},
doi = {10.1038/44565},
eprint = {arXiv:1408.1149},
isbn = {0028-0836 (Print){\$}\backslash{\$}r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
keywords = {Algorithms,Face,Humans,Learning,Models,Neurological,Perception,Perception: physiology,Semantics},
number = {6755},
pages = {788--791},
pmid = {10548103},
title = {{Learning the parts of objects by non-negative matrix factorization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10548103},
volume = {401},
year = {1999}
}
@article{zhang2015,
abstract = {In computer vision and pattern recognition researches, the studied objects are often characterized by multiple feature representations with high dimensionality, thus it is essential to encode that multiview feature into a unified and discriminative embedding that is optimal for a given task. To address this challenge, this paper proposes an ensemble manifold regularized sparse low-rank approximation (EMR-SLRA) algorithm for multiview feature embedding. The EMR-SLRA algorithm is based on the framework of least-squares component analysis, in particular, the low dimensional feature representation and the projection matrix are obtained by the low-rank approximation of the concatenated multiview feature matrix. By considering the complementary property among multiple features, EMR-SLRA simultaneously enforces the ensemble manifold regularization on the output feature embedding. In order to further enhance its robustness against the noise, the group sparsity is introduced into the objective formulation to impose direct noise reduction on the input multiview feature matrix. Since there is no closed-form solution for EMR-SLRA, this paper provides an efficient EMR-SLRA optimization procedure to obtain the output feature embedding. Experiments on the pattern recognition applications confirm the effectiveness of the EMR-SLRA algorithm compare with some other multiview feature dimensionality reduction approaches.},
author = {Zhang, Lefei and Zhang, Qian and Zhang, Liangpei and Tao, Dacheng and Huang, Xin and Du, Bo},
doi = {10.1016/j.patcog.2014.12.016},
file = {:home/samir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2015 - Ensemble manifold regularized sparse low-rank approximation for multiview feature embedding.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Ensemble manifold regularization,Feature extraction,Group sparsity,Low-rank matrix approximation,Multiview},
number = {10},
pages = {3102--3112},
title = {{Ensemble manifold regularized sparse low-rank approximation for multiview feature embedding}},
volume = {48},
year = {2015}
}
@misc{kaufman1987,
abstract = {When partitioning a set of objects into k clusters, the main objective is to find clusters the objects of which show a high degree of similarity. There are several possible criteria for quantifying this objective, the best known of which are based on sums of squares. An alternative approach, used in the k-medoid method, is of the L 1 type. It searches for k `representative' objects, called medoids, which minimize the average dissimilarity of all objects of the data set to the nearest medoid. A cluster is then defined as the set of objects which have been assigned to the same medoid. An algorithm for this method has been implemented in the PAM program which is described in the paper. The k-medoid method offers the advantage of not requiring the actual measurement data, as it can also be applied to a collection of dissimilarities. Such dissimilarities are for example subjective assessments of relationships between objects, in which case no measurements exist. An example of the latter type is analyzed. Furthermore, the k-medoid method is less susceptible to outlying values},
author = {Kaufman, L and Rousseeuw, P J},
booktitle = {Statistical Data Analysis Based on the L 1-Norm and Related Methods. First International Conference},
isbn = {0444702733},
issn = {0022-3999},
pages = {405--416416},
title = {{Clustering by means of medoids}},
year = {1987}
}
@article{caruana1997,
abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents newresults forMTLwith k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learningworks, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
author = {Caruana, Rich},
doi = {10.1023/A:1007379606734},
isbn = {1461375274},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel,multitask learning,parallel transfer,regression,supervised learning},
number = {1},
pages = {41--75},
pmid = {20421687},
title = {{Multitask Learning}},
url = {http://www.cs.cornell.edu/{\%}7B{~}{\%}7Dcaruana/mlj97.pdf},
volume = {28},
year = {1997}
}
@article{strehl2002,
abstract = {This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We rst identify several application scenarios for the resultant `knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three e ective and efficient techniques for obtaining high-quality combiners (consensus functions). The rst combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively di erent application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.},
author = {Strehl, Alexander and Ghosh, Joydeep},
doi = {10.1162/153244303321897735},
isbn = {0262511290},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {cluster analysis,clustering,clustering ensemble,consensus clustering,consensus functions,ensemble,knowledge reuse,multi-learner,mutual information,partitioning,systems,unsupervised learning},
pages = {583--617},
pmid = {21423337},
title = {{Cluster Ensembles – A Knowledge Reuse Framework for Combining Multiple Partitions}},
volume = {3},
year = {2002}
}
@article{erven2014,
abstract = {R{\{}{\'{e}}{\}}nyi divergence is related to R{\{}{\'{e}}{\}}nyi entropy much like Kullback-Leibler divergence is related to Shannon's entropy, and comes up in many settings. It was introduced by R{\{}{\'{e}}{\}}nyi as a measure of information that satisfies almost the same axioms as Kullback-Leibler divergence, and depends on a parameter that is called its order. In particular, the R{\{}{\'{e}}{\}}nyi divergence of order 1 equals the Kullback-Leibler divergence. We review and extend the most important properties of R{\{}{\'{e}}{\}}nyi divergence and Kullback- Leibler divergence, including convexity, continuity, limits of {\$}\sigma{\$}-algebras, and the relation of the special order 0 to the Gaussian dichotomy and contiguity. We also show how to generalize the Pythagorean inequality to orders different from 1, and we extend the known equivalence between channel capacity and minimax redundancy to continuous channel inputs (for all orders) and present several other minimax results.},
archivePrefix = {arXiv},
arxivId = {1206.2459},
author = {van Erven, T and Harremoes, P},
doi = {10.1109/TIT.2014.2320500},
eprint = {1206.2459},
isbn = {0018-9448 VO - 60},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {7},
pages = {3797--3820},
title = {{Renyi Divergence and Kullback-Leibler Divergence}},
url = {http://ieeexplore.ieee.org/ielx7/18/6832684/06832827.pdf?tp={\%}7B{\&}{\%}7Darnumber=6832827{\%}7B{\&}{\%}7Disnumber=6832684{\%}7B{\%}25{\%}7D5Cnhttp://ieeexplore.ieee.org/xpls/abs{\%}7B{\_}{\%}7Dall.jsp?arnumber=6832827{\%}7B{\&}{\%}7Dtag=1},
volume = {60},
year = {2014}
}
@article{zhang2016,
abstract = {In data mining and pattern recognition area, the learned objects are often represented by the multiple features from various of views. How to learn an efficient and effective feature embedding for the subsequent learning tasks? In this paper, we address this issue by providing a novel multi-task multiview feature embedding (MMFE) framework. The MMFE algorithm is based on the idea of low-rank approximation, which suggests that the observed multiview feature matrix is approximately represented by the low-dimensional feature embedding multiplied by a projection matrix. In order to fully consider the particular role of each view to the multiview feature embedding, we simultaneously suggest the multitask learning scheme and ensemble manifold regularization into the MMFE algorithm to seek the optimal projection. Since the objection function of MMFE is multi-variable and non-convex, we further provide an iterative optimization procedure to find the available solution. Two real world experiments show that the proposed method outperforms single-task-based as well as state-of-the-art multiview feature embedding methods for the classification problem.},
author = {Zhang, Qian and Zhang, Lefei and Du, Bo and Zheng, Wei and Bian, Wei and Tao, Dacheng},
doi = {10.1109/ICDM.2015.82},
isbn = {9781467395038},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Classification,Dimension reduction,Multitask,Multiview},
pages = {1105--1110},
title = {{MMFE: Multitask multiview feature embedding}},
volume = {2016-Janua},
year = {2016}
}
@article{maaten2009,
abstract = {The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space. We evaluate the performance of parametric t-SNE in experiments on two datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.},
author = {Maaten, Laurens Van Der},
issn = {15324435},
journal = {JMLR Proceedings vol. 5 (AISTATS)},
pages = {384--391},
title = {{Learning a Parametric Embedding by Preserving Local Structure}},
year = {2009}
}
@article{gallier2010,
abstract = {In this note, we provide some details and proofs of some results from Appendix A.5 (especially Section A.5.5) of Convex Optimization by Boyd and Vandenberghe.},
author = {Gallier, Jean},
journal = {Complement},
pages = {1--12},
title = {{The Schur Complement and Symmetric Positive Semidefinite ( and Definite ) Matrices}},
url = {http://www.cis.upenn.edu/{\%}7B{~}{\%}7Djean/schur-comp.pdf},
year = {2010}
}
@article{chua2009,
abstract = {This paper introduces a web image dataset created by NUS's Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval.},
author = {Chua, Tat-Seng and Tang, Jinhui and Hong, Richang and Li, Haojie and Luo, Zhiping and Zheng, Yantao},
doi = {10.1145/1646396.1646452},
isbn = {9781605584805},
journal = {Acmmm},
keywords = {Flickr,annotation,retrieval,tag refinement,training set construction,web image},
pages = {1},
title = {{NUS-WIDE: A Real-World Web Image Database from National University of Singapore}},
url = {http://dl.acm.org/citation.cfm?id=1646396.1646452},
year = {2009}
}
@article{xia2010,
abstract = {In computer vision and multimedia search, it is common to use multiple features from different views to represent an object. For example, to well characterize a natural scene image, it is essential to find a set of visual features to represent its color, texture, and shape information and encode each feature into a vector. Therefore, we have a set of vectors in different spaces to represent the image. Conventional spectral-embedding algorithms cannot deal with such datum directly, so we have to concatenate these vectors together as a new vector. This concatenation is not physically meaningful because each feature has a specific statistical property. Therefore, we develop a new spectral-embedding algorithm, namely, multiview spectral embedding (MSE), which can encode different features in different ways, to achieve a physically meaningful embedding. In particular, MSE finds a low-dimensional embedding wherein the distribution of each view is sufficiently smooth, and MSE explores the complementary property of different views. Because there is no closed-form solution for MSE, we derive an alternating optimization-based iterative algorithm to obtain the low-dimensional embedding. Empirical evaluations based on the applications of image retrieval, video annotation, and document clustering demonstrate the effectiveness of the proposed approach.},
author = {Xia, Tian and Tao, Dacheng and Mei, Tao and Zhang, Yongdong},
doi = {10.1109/TSMCB.2009.2039566},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Dimensionality reduction,multiple views,spectral embedding},
number = {6},
pages = {1438--1446},
pmid = {20172832},
title = {{Multiview spectral embedding}},
volume = {40},
year = {2010}
}
@article{giles1998,
abstract = {We present CiteSeer: an autonomous citation indexing system which indexes academic literature in electronic format (e.g. Postscript files on theWeb). CiteSeer understands how to parse citations, identify citations to the same paper in different formats, and identify the context of citations in the body of articles. CiteSeer provides most of the advantages of traditional (manually constructed) citation indexes (e.g. the ISI citation indexes), including: literature retrieval by following citation links (e.g. by providing a list of papers that cite a given paper), the evaluation and ranking of papers, authors, journals, etc. based on the number of citations, and the identification of research trends. CiteSeer has many advantages over traditional citation indexes, including the ability to create more up-to-date databases which are not limited to a preselected set of journals or restricted by journal publication delays, completely autonomous operation with a corresponding reduction in cost, and powerful interactive browsing of the literature using the context of citations. Given a particular paper of interest, CiteSeer can display the context of how the paper is cited in subsequent publications. This context may contain a brief summary of the paper, another author's response to the paper, or subsequentwork which builds upon the original article. CiteSeer allows the location of papers by keyword search or by citation links. Papers related to a given paper can be located using common citation information or word vector similarity. CiteSeer will soon be available for public use.},
author = {{C. Giles, K.D. Bollacker}, S Lawrence},
doi = {10.1145/276675.276685},
isbn = {0897919653},
journal = {Digital Libraries 98: Third ACM Conference on Digital Libraries},
keywords = {bibliometrics,citation context,citation indexing,literature search},
pages = {89--98},
title = {{CiteSeer: An automatic citation indexing system}},
url = {https://clgiles.ist.psu.edu/papers/DL-1998-citeseer.pdf},
year = {1998}
}
@article{xiang2012,
abstract = {This paper presents a framework of discriminative least squares regression (LSR) for multiclass classification and feature selection. The core idea is to enlarge the distance between different classes under the conceptual framework of LSR. First, a technique called {\$}\epsilon{\$}-dragging is introduced to force the regression targets of different classes moving along opposite directions such that the distances between classes can be enlarged. Then, the {\$}\epsilon{\$}-draggings are integrated into the LSR model for multiclass classification. Our learning framework, referred to as discriminative LSR, has a compact model form, where there is no need to train two-class machines that are independent of each other. With its compact form, this model can be naturally extended for feature selection. This goal is achieved in terms of L2,1 norm of matrix, generating a sparse learning model for feature selection. The model for multiclass classification and its extension for feature selection are finally solved elegantly and efficiently. Experimental evaluation over a range of benchmark datasets indicates the validity of our method.},
author = {Xiang, Shiming and Nie, Feiping and Meng, Gaofeng and Pan, Chunhong and Zhang, Changshui},
doi = {10.1109/TNNLS.2012.2212721},
issn = {2162237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Feature selection,least squares regression,multiclass classification,sparse learning},
number = {11},
pages = {1738--1754},
pmid = {24808069},
title = {{Discriminative least squares regression for multiclass classification and feature selection}},
volume = {23},
year = {2012}
}
@article{Xie2011,
abstract = {Dimension reduction has been widely used in real-world applications such as image retrieval and document classification. In many scenarios, different features (or multiview data) can be obtained, and how to duly utilize them is a challenge. It is not appropriate for the conventional concatenating strategy to arrange features of different views into a long vector. That is because each view has its specific statistical property and physical interpretation. Even worse, the performance of the concatenating strategy will deteriorate if some views are corrupted by noise. In this paper, we propose a multiview stochastic neighbor embedding (m-SNE) that systematically integrates heterogeneous features into a unified representation for subsequent processing based on a probabilistic framework. Compared with conventional strategies, our approach can automatically learn a combination coefficient for each view adapted to its contribution to the data embedding. This combination coefficient plays an important role in utilizing the complementary information in multiview data. Also, our algorithm for learning the combination coefficient converges at a rate of {\{}{\textless}{\}}formula formulatype="inline"{\{}{\textgreater}{\}}{\{}{\textless}{\}}tex Notation="TeX"{\{}{\textgreater}{\}}{\{}{\$}{\}}O(1/k{\{}{\^{}}{\}}{\{}{\{}{\}}2{\{}{\}}{\}}){\{}{\$}{\}}{\{}{\textless}{\}}/tex{\{}{\textgreater}{\}}{\{}{\textless}{\}}/formula{\{}{\textgreater}{\}}, which is the optimal rate for smooth problems. Experiments on synthetic and real data sets suggest the effectiveness and robustness of m-SNE for data visualization, image retrieval, object categorization, and scene recognition.},
author = {Xie, Bo and Mu, Yang and Tao, Dacheng and Huang, Kaiqi},
doi = {10.1109/TSMCB.2011.2106208},
isbn = {3642175368},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Dimension reduction,image retrieval,multiview learning,stochastic neighbor embedding},
number = {4},
pages = {1088--1096},
pmid = {21296709},
title = {{M-SNE: Multiview stochastic neighbor embedding}},
volume = {41},
year = {2011}
}
@article{yuan2006,
abstract = {Summary. We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
author = {Yuan, Ming and Lin, Yi},
doi = {10.1111/j.1467-9868.2005.00532.x},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Analysis of variance,Lasso,Least angle regression,Non-negative garrotte,Piecewise linear solution path},
number = {1},
pages = {49--67},
pmid = {11161800},
title = {{Model selection and estimation in regression with grouped variables}},
volume = {68},
year = {2006}
}
@article{delatorre2012,
abstract = {Over the last century, Component Analysis (CA) methods such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Canonical Correlation Analysis (CCA), Locality Preserving Projections (LPP), and Spectral Clustering (SC) have been extensively used as a feature extraction step for modeling, classification, visualization, and clustering. CA techniques are appealing because many can be formulated as eigen-problems, offering great potential for learning linear and nonlinear representations of data in closed-form. However, the eigen-formulation often conceals important analytic and computational drawbacks of CA techniques, such as solving generalized eigen-problems with rank deficient matrices (e.g., small sample size problem), lacking intuitive interpretation of normalization factors, and understanding commonalities and differences between CA methods. This paper proposes a unified least-squares framework to formulate many CA methods. We show how PCA, LDA, CCA, LPP, SC, and its kernel and regularized extensions correspond to a particular instance of least-squares weighted kernel reduced rank regression (LS--WKRRR). The LS-WKRRR formulation of CA methods has several benefits: 1) provides a clean connection between many CA techniques and an intuitive framework to understand normalization factors; 2) yields efficient numerical schemes to solve CA techniques; 3) overcomes the small sample size problem; 4) provides a framework to easily extend CA methods. We derive weighted generalizations of PCA, LDA, SC, and CCA, and several new CA techniques.},
author = {{De La Torre}, Fernando},
doi = {10.1109/TPAMI.2011.184},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Canonical correlation analysis,Dimensionality reduction,K-means,Kernel methods,Linear discriminant analysis,Principal component analysis,Reduced rank regression,Spectral clustering},
number = {6},
pages = {1041--1055},
pmid = {21911913},
title = {{A least-squares framework for component analysis}},
volume = {34},
year = {2012}
}
@inproceedings{liao1997,
abstract = {The discretization error analysis for the Zernike moment computing is carried out. Based on the proposed new techniques to increase the numerical accuracy of the Zernike moments, the image reconstruction power of Zernike moment descriptors is examined},
author = {Liao, S X and Pawlak, M},
booktitle = {Electrical and Computer Engineering, 1997. Engineering Innovation: Voyage of Discovery. IEEE 1997 Canadian Conference on},
doi = {10.1109/CCECE.1997.608334},
isbn = {0840-7789},
issn = {0840-7789},
keywords = {Analog computers,Character recognition,Pixel,Polynomials,Redundancy,Shape,Zernike moment computing,Zernike moment descriptors,discretization error analysis,error analysis,image analysis,image recognition,image reconstruction,numerical accuracy},
pages = {700----703 vol.2},
title = {{Image analysis with Zernike moment descriptors}},
volume = {2},
year = {1997}
}
@article{smith1981,
abstract = {The identification of maximally homologous subsequences among sets of long sequences is an important problem in molecular sequence analysis. The problem is straightforward only if one restricts consideration to contiguous subsequences (segments) containing no internal deletions or insertions. The more general problem has its solution in an extension of sequence metrics (Sellers 1974; Waterman et al., 1976) developed to measure the minimum number of “events” required to convert one sequence into another. These developments in the modern sequence analysis began with the heuristic homology algorithm of Needleman {\{}{\&}{\}} Wunsch (1970) which first introduced an iterative matrix method of calculation. Numerous other heuristic algorithms have been suggested including those of Fitch (1966) and Dayhoff (1969). More mathemat- ically rigorous algorithms were suggested by Sankoff (1972), Reichert et al. (1973) and Beyer et al. (1979), but these were generally not biologically satisfying or interpretable. Success came with Sellers (1974) development of a true metric mewure of the distance between sequences. This metric was later generalized by Waterman et al. (1976) to include deletions/insertions of arbitrary length. This metric represents the minimum number of “mutational events” required to convert one sequence into another. It is of interest to note that Smith et al. (1980) have recently shown that under some conditions the generalized Sellers metric is equivalent to the original homology algorithm of Needleman {\{}{\&}{\}} Wunsch (1970). In this letter we extend the above ideas to find a pair of segments, one from each of two long sequences, such that there is no other pair of segments with greater similarity (homology). The similarity measure used here allows for arbitrary length deletions and insertions},
author = {Smith, T F and Waterman, M S},
doi = {10.1016/0022-2836(81)90087-5},
isbn = {0022-2836},
issn = {00222836},
journal = {Journal of Molecular Biology},
number = {1},
pages = {195--197},
pmid = {7265238},
title = {{Identification of common molecular subsequences}},
volume = {147},
year = {1981}
}
@article{murtagh2001,
abstract = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.},
archivePrefix = {arXiv},
arxivId = {1105.0121},
author = {Murtagh, Fionn and Contreras, Pedro},
doi = {10.1007/s00181-009-0254-1},
eprint = {1105.0121},
isbn = {0018100902},
issn = {03777332},
journal = {Computer},
keywords = {()},
number = {2},
pages = {1--21},
title = {{Methods of Hierarchical Clustering}},
url = {http://arxiv.org/abs/1105.0121},
volume = {38},
year = {2011}
}
@article{davies1979,
abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
author = {Davies, D L and Bouldin, D W},
doi = {10.1109/TPAMI.1979.4766909},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {224--227},
pmid = {21868852},
title = {{A cluster separation measure}},
volume = {1},
year = {1979}
}
@article{hsu2002,
abstract = {Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such "all-together" methods. We then compare their performance with three methods based on binary classifications: "one-against-all," "one-against-one," and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the "one-against-one" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors.},
author = {Hsu, Chih Wei and Lin, Chih Jen},
doi = {10.1109/72.991427},
file = {:home/samir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu, Lin - 2002 - A comparison of methods for multiclass support vector machines.pdf:pdf},
isbn = {3-540-32026-1},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Decomposition methods,Multiclass classification,Support vector machines (SVMs)},
number = {2},
pages = {415--425},
pmid = {18244442},
title = {{A comparison of methods for multiclass support vector machines}},
volume = {13},
year = {2002}
}
@article{tang1998,
abstract = {We use a multilevel dominant eigenvector estimation algorithm to develop a new run-length texture feature extraction algorithm that preserves much of the texture information in run-length matrices and significantly improves image classification accuracy over traditional run-length techniques. The advantage of this approach is demonstrated experimentally by the classification of two texture data sets. Comparisons with other methods demonstrate that the run-length matrices contain great discriminatory information and that a good method of extracting such information is of paramount importance to successful classification.},
author = {Tang, Xiaoou},
doi = {10.1109/83.725367},
isbn = {1057-7149 (Print){\$}\backslash{\$}r1057-7149 (Linking)},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Pattern classification,Run-length matrix,Texture analysis},
number = {11},
pages = {1602--1609},
pmid = {18276225},
title = {{Texture information in run-length matrices}},
volume = {7},
year = {1998}
}
@article{shao2016,
author = {Shao, Ling and Liu, Li and Yu, Mengyang},
doi = {10.1007/s11263-015-0861-6},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Dimensionality reduction,Human action recognition,Multiple view fusion,Sequential distance learning,Spectral coding},
number = {2},
pages = {115--129},
publisher = {Springer US},
title = {{Kernelized Multiview Projection for Robust Action Recognition}},
volume = {118},
year = {2016}
}
@article{jolliffe2002,
abstract = {seems like a great book on PCA - it shows the connection between PCA and SVD; talks about how to choose the number of eigenvectors to keep; discusses outlier detection; uses PCA for stock prices (Dow Jones)},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
doi = {10.1002/0470013192.bsa501},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-470-86080-9},
issn = {00401706},
journal = {Encyclopedia of Statistics in Behavioral Science},
keywords = {file-import-08-04-21,imported},
pages = {1580--1584},
pmid = {21435900},
title = {{Principal Component Analysis}},
url = {http://www.google.com/search?client=safari{\%}7B{\&}{\%}7Drls=en-us{\%}7B{\&}{\%}7Dq=Principal+Component+Analysis{\%}7B{\&}{\%}7Die=UTF-8{\%}7B{\&}{\%}7Doe=UTF-8},
volume = {3},
year = {2005}
}
@article{laan2003,
abstract = {Kaufman and Rousseeuw (1990) proposed a clustering algorithm Partitioning Around Medoids (PAM) which maps a distance matrix into a specified number of clusters. A particularly nice property is that PAM allows clustering with respect to any specified distance metric. In addition, the medoids are robust representations of the cluster centers, which is particularly important in the common context that many elements do not belong well to any cluster. Based on our experience in clustering gene expression data, we have noticed that PAM does have problems recognizing relatively small clusters in situations where good partitions around medoids clearly exist. In this paper, we propose to partition around medoids by maximizing a criteria "Average Silhouette" defined by Kaufman and Rousseeuw (1990). We also propose a fast-to-compute approximation of "Average Silhouette". We implement these two new partitioning around medoids algorithms and illustrate their performance relative to existing partitioning methods in sim...},
author = {der Laan, Mark Van and Pollard, Katherine and Bryan, Jennifer},
doi = {10.1080/0094965031000136012},
isbn = {0094-9655},
issn = {0094-9655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Cluster Analysis,Gene Expression,Parameter,Silhouette},
number = {8},
pages = {575--584},
title = {{A new partitioning around medoids algorithm}},
volume = {73},
year = {2003}
}
@article{desideri2012,
abstract = {One considers the context of the concurrent optimization of several criteria J i(Y) (i=1, . ., n), supposed to be smooth functions of the design vector Y∈RN (n≤N). An original constructive solution is given to the problem of identifying a descent direction common to all criteria when the current design-point Y 0 is not Pareto-optimal. This leads us to generalize the classical steepest-descent method to the multiobjective context by utilizing this direction for the descent. The algorithm is then proved to converge to a Pareto-stationary design-point. {\{}{\textcopyright}{\}} 2012 Acad{\{}{\'{e}}{\}}mie des sciences.},
author = {D{\'{e}}sid{\'{e}}ri, Jean Antoine},
doi = {10.1016/j.crma.2012.03.014},
isbn = {1631-073X},
issn = {1631073X},
journal = {Comptes Rendus Mathematique},
number = {5-6},
pages = {313--318},
title = {{Multiple-gradient descent algorithm (MGDA) for multiobjective optimization}},
volume = {350},
year = {2012}
}
@article{mccallum1998,
abstract = {Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes—providing on average a 27{\{}{\%}{\}} reduction in error over the multi-variate Bernoulli model at any vocabulary size. Introduction},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {McCallum, Andres and Nigam, Kamal},
doi = {10.1.1.46.1529},
eprint = {0-387-31073-8},
isbn = {0897915240},
issn = {0343-6993},
journal = {AAAI/ICML-98 Workshop on Learning for Text Categorization},
pages = {41--48},
pmid = {20236947},
title = {{A Comparison of Event Models for Naive Bayes Text Classification}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.9324{\%}7B{\&}{\%}7Drep=rep1{\%}7B{\&}{\%}7Dtype=pdf},
year = {1998}
}
@article{duygulu2002,
abstract = {We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Duygulu, Pinar and Barnard, Kobus and {De Freitas}, J and Forsyth, David and de Freitas, Nando},
doi = {10.1007/3-540-47979-1},
eprint = {arXiv:1011.1669v3},
isbn = {9783540437482},
issn = {0302-9743 (Print) 1611-3349 (Online)},
journal = {Proc. European Conference on Computer Vision (ECCV)},
keywords = {correspondence,em algorithm,object recognition},
pages = {97--112},
pmid = {25246403},
title = {{Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary}},
url = {http://ci.nii.ac.jp/naid/10020185192/{\%}7B{\%}25{\%}7D5Cnhttp://www.springerlink.com/index/DMX4GC895QRRJKT4.pdf},
volume = {2353},
year = {2002}
}
@inproceedings{Cai2011,
abstract = {In recent years, more and more visual descriptors have been proposed to describe objects and scenes appearing in images. Different features describe different aspects of the visual characteristics. How to combine these heterogeneous features has become an increasing critical problem. In this paper, we propose a novel approach to unsupervised integrate such heterogeneous features by performing multi-modal spectral clustering on unlabeled images and unsegmented images. Considering each type of feature as one modal, our new multi-modal spectral clustering (MMSC) algorithm is to learn a commonly shared graph Laplacian matrix by unifying different modals (image features). A non-negative relaxation is also added in our method to improve the robustness and efficiency of image clustering. We applied our MMSC method to integrate five types of popularly used image features, including SIFT, HOG, GIST, LBP, CENTRIST and evaluated the performance by two benchmark data sets: Caltech-101 and MSRC-v1. Compared with existing unsupervised scene and object categorization methods, our approach always achieves superior performances measured by three standard clustering evaluation metrices.},
author = {Cai, Xiao and Nie, Feiping and Huang, Heng and Kamangar, Farhad},
booktitle = {Cvpr},
doi = {10.1109/ICCV.2013.218},
isbn = {9781457703942},
issn = {10636919},
keywords = {multiview learning;feature fusion},
pages = {1977--1984},
title = {{Heterogeneous Image Features Integration via Multi-View Spectral Clustering}},
year = {2011}
}
@article{greene2006,
abstract = {In supervised kernel methods, it has been observed that the performance of the SVM classifier is poor in cases where the diagonal entries of the Gram matrix are large relative to the off-diagonal entries. This problem, referred to as diagonal dominance, often occurs when certain kernel functions are applied to sparse high-dimensional data, such as text corpora. In this paper we investigate the implications of diagonal dominance for unsupervised kernel methods, specifically in the task of document clustering. We propose a selection of strategies for addressing this issue, and evaluate their effectiveness in producing more accurate and stable clusterings.},
author = {Greene, Derek and Cunningham, Padraig},
doi = {10.1145/1143844.1143892},
isbn = {1595933832},
journal = {Proceedings of the 23rd International Conference on Machine Learning (ICML'06)},
pages = {377--384},
title = {{Practical solutions to the problem of diagonal dominance in kernel document clustering}},
url = {http://mlg.ucd.ie/files/publications/greene06icml.pdf},
year = {2006}
}
@article{costapereira2014,
abstract = {The problem of cross-modal retrieval from multimedia repositories is considered. This problem addresses the design of retrieval systems that support queries across content modalities, for example, using an image to search for texts. A mathematical formulation is proposed, equating the design of cross-modal retrieval systems to that of isomorphic feature spaces for different content modalities. Two hypotheses are then investigated regarding the fundamental attributes of these spaces. The first is that low-level cross-modal correlations should be accounted for. The second is that the space should enable semantic abstraction. Three new solutions to the cross-modal retrieval problem are then derived from these hypotheses: correlation matching (CM), an unsupervised method which models cross-modal correlations, semantic matching (SM), a supervised technique that relies on semantic representation, and semantic correlation matching (SCM), which combines both. An extensive evaluation of retrieval performance is conducted to test the validity of the hypotheses. All approaches are shown successful for text retrieval in response to image queries and vice versa. It is concluded that both hypotheses hold, in a complementary form, although evidence in favor of the abstraction hypothesis is stronger than that for correlation.},
author = {{Costa Pereira}, Jose and Coviello, Emanuele and Doyle, Gabriel and Rasiwasia, Nikhil and Lanckriet, Gert R G and Levy, Roger and Vasconcelos, Nuno},
doi = {10.1109/TPAMI.2013.142},
isbn = {0162-8828 VO - 36},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multimedia,content-based retrieval,cross-modal,image and text,kernel correlation,logistic regression,multimodal,retrieval model,semantic spaces},
number = {3},
pages = {521--535},
pmid = {24457508},
title = {{On the role of correlation and abstraction in cross-modal multimedia retrieval}},
volume = {36},
year = {2014}
}
@article{deb2002,
abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN3) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed},
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T},
doi = {10.1109/4235.996017},
isbn = {1089-778X VO - 6},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Constraint handling,Elitism,Genetic algorithms,Multicriterion decision making,Multiobjective optimization,Pareto-optimal solutions},
number = {2},
pages = {182--197},
title = {{A fast and elitist multiobjective genetic algorithm: NSGA-II}},
volume = {6},
year = {2002}
}
@article{nene1996,
abstract = {Columbia Object Image Library (COIL-20) is a database of gray-scale images of 20 objects. The objects were placed on a motorized turntable against a black background. The turntable was rotated through 360 degrees to vary object pose with respect to a fixed camera. Images of the objects were taken at pose intervals of 5 degrees. This corresponds to 72 images per object. The database has two sets of images. The first set contains 720 unprocessed images of 10 objects. The second contains 1,440 size normalized images of 20 objects. COIL-20 is available online via ftp. i 1 Introduction We have constructed a database of 1,440 grayscale images of 20 objects (72 images per object). The objects have a wide variety of complex geometric and reflectance characteristics (see figure 1(a)). The database, called Columbia Object Image Library (COIL-20), was used in a real-time 20 object recognition system Murase and Nayar-1995 . Figure 1(b) shows an object from the database being placed in front...},
author = {Nene, S and Nayar, S and Murase, H},
journal = {Technical Report},
pages = {223--303},
title = {{Columbia Object Image Library (COIL-20)}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.5914},
volume = {95},
year = {1996}
}
@article{hardoon2004,
abstract = {We present a general method using kernel canonical correlation analysis to learn a semantic representation to web images and their associated text. The semantic space provides a common representation and enables a comparison between the text and images. In the experiments, we look at two approaches of retrieving images based on only their content from a text query. We compare orthogonalization approaches against a standard cross-representation retrieval technique known as the generalized vector space model.},
author = {Hardoon, David R and Szedmak, Sandor and Shawe-Taylor, John},
doi = {10.1162/0899766042321814},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
number = {12},
pages = {2639--64},
pmid = {15516276},
title = {{Canonical correlation analysis: an overview with application to learning methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15516276},
volume = {16},
year = {2004}
}
@book{kruskal1978,
abstract = {The theory of multidimensional scaling arose and grew within the field of the behavioral sciences and now covers several statistical techniques that are widely used in many disciplines. Intended for readers of varying backgrounds, this book comprehensively covers the area while serving as an introduction to the mathematical ideas behind the various techniques of multidimensional scaling.},
author = {Cox, Trevor F and Cox, Michael A A},
booktitle = {New York},
doi = {10.1201/9781420036121},
isbn = {1584880945},
pages = {328},
title = {{Multidimensional Scaling, Second Edition}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/1584880945{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=ZzmIPcEXPf0C{\&}oi=fnd{\&}pg=PA5{\&}dq=Multidimensional+Scaling{\&}ots=YRZSc6R1Gy{\&}sig=lSgXPWoV2myWDMZvFbHilXieDAc{\%}5Cnhttp://books.googl},
volume = {88},
year = {2000}
}
@article{Hinton2002,
abstract = {Abstract We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high- ... $\backslash$n},
author = {Hinton, Geoffrey E and Roweis, Sam T},
doi = {http://books.nips.cc/papers/files/nips15/AA45.pdf},
isbn = {0262025507},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {833--840},
title = {{Stochastic neighbor embedding}},
year = {2002}
}
@article{bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
isbn = {9783540338321},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
number = {3},
pages = {346--359},
pmid = {16081019},
title = {{Speeded-Up Robust Features (SURF)}},
volume = {110},
year = {2008}
}
@article{lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G.},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
number = {[8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
volume = {2},
year = {1999}
}
@article{kruskal1964b,
abstract = {We describe the numerical methods required in our approach to multi-dimensional scaling. The rationale of this approach has appeared previously.},
author = {Kruskal, J. B.},
doi = {10.1007/BF02289694},
isbn = {0033-3123},
issn = {00333123},
journal = {Psychometrika},
number = {2},
pages = {115--129},
title = {{Nonmetric multidimensional scaling: A numerical method}},
volume = {29},
year = {1964}
}
@article{zhang2009,
abstract = {Spectral analysis-based dimensionality reduction algorithms are important and have been popularly applied in data mining and computer vision applications. To date many algorithms have been developed, e.g., principal component analysis, locally linear embedding, Laplacian eigenmaps, and local tangent space alignment. All of these algorithms have been designed intuitively and pragmatically, i.e., on the basis of the experience and knowledge of experts for their own purposes. Therefore, it will be more informative to provide a systematic framework for understanding the common properties and intrinsic difference in different algorithms. In this paper, we propose such a framework, named "patch alignment,rdquo which consists of two stages: part optimization and whole alignment. The framework reveals that (1) algorithms are intrinsically different in the patch optimization stage and (2) all algorithms share an almost identical whole alignment stage. As an application of this framework, we develop a new dimensionality reduction algorithm, termed discriminative locality alignment (DLA), by imposing discriminative information in the part optimization stage. DLA can (1) attack the distribution nonlinearity of measurements; (2) preserve the discriminative ability; and (3) avoid the small-sample-size problem. Thorough empirical studies demonstrate the effectiveness of DLA compared with representative dimensionality reduction algorithms.},
author = {Zhang, Tianhao and Tao, Dacheng and Li, Xuelong and Yang, Jie},
doi = {10.1109/TKDE.2008.212},
isbn = {1041-4347 VO - 21},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Dimensionality reduction,Discriminative locality alignment,Patch alignment,Spectral analysis},
number = {9},
pages = {1299--1313},
title = {{Patch alignment for dimensionality reduction}},
volume = {21},
year = {2009}
}
@misc{maaten2008,
abstract = {We present a new technique called " t-SNE " that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey and van der Maaten, Geoffrey Hinton},
booktitle = {Journal of Machine Learning Research},
doi = {10.1007/s10479-011-0841-3},
eprint = {1307.1662},
isbn = {1532-4435},
issn = {1532-4435},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing Data using t-SNE}},
volume = {9},
year = {2008}
}
@article{berndt1994,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, Donald and Clifford, James},
isbn = {0-929280-73-3},
journal = {Workshop on Knowledge Knowledge Discovery in Databases},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
pages = {359--370},
title = {{Using dynamic time warping to find patterns in time series}},
url = {http://www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf},
volume = {398},
year = {1994}
}
@article{lanckriet2004,
author = {Lanckriet, Gert R G and Bie, Tijl De and Cristianini, Nello and Jordan, Michael I and Noble, William Stafford},
doi = {10.1093/bioinformatics/bth294},
file = {:home/samir/Dropbox/Doctorado/201608 MVMDS paper/refs{\_}datasets/lanckriet-genomic-data-fusion-2004.pdf:pdf},
number = {16},
pages = {2626--2635},
title = {{A statistical framework for genomic data fusion}},
volume = {20},
year = {2004}
}
@article{parameswaran2010,
abstract = {Multi-task learning (MTL) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations. One of the most prominent multi-task learning algorithms is an extension to svms by Evgeniou et al. Although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes. This paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (lmnn) algorithm to the MTL paradigm. Instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multitask learning. We evaluate the resulting multi-task lmnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task kNN under several metrics and state-of-the-art MTL classifiers.},
author = {Parameswaran, Shibin and Weinberger, Kilian Q},
isbn = {9781617823800},
journal = {Nips},
pages = {1--9},
title = {{Large Margin Multi-Task Metric Learning}},
url = {http://papers.nips.cc/paper/3935-large-margin-multi-task-metric-learning.pdf},
year = {2010}
}
@article{Lee2013,
abstract = {Stochastic neighbor embedding (SNE) and its variants are methods of dimensionality reduction (DR) that involve normalized softmax similarities derived from pairwise distances. These methods try to reproduce in the low-dimensional embedding space the similarities observed in the high-dimensional data space. Their outstanding experimental results, compared to previous state-of-the-art methods, originate from their capability to foil the curse of dimensionality. Previous work has shown that this immunity stems partly from a property of shift invariance that allows appropriately normalized softmax similarities to mitigate the phenomenon of norm concentration. This paper investigates a complementary aspect, namely, the cost function that quantifies the mismatch between similarities computed in the high- and low-dimensional spaces. Stochastic neighbor embedding and its variant . t-SNE rely on a single Kullback-Leibler divergence, whereas a weighted mixture of two dual KL divergences is used in neighborhood retrieval and visualization (NeRV). We propose in this paper a different mixture of KL divergences, which is a scaled version of the generalized Jensen-Shannon divergence. We show experimentally that this divergence produces embeddings that better preserve small . K-ary neighborhoods, as compared to both the single KL divergence used in SNE and . t-SNE and the mixture used in NeRV. These results allow us to conclude that future improvements in similarity-based DR will likely emerge from better definitions of the cost function. ?? 2013 Elsevier B.V.},
author = {Lee, John A. and Renard, Emilie and Bernard, Guillaume and Dupont, Pierre and Verleysen, Michel},
doi = {10.1016/j.neucom.2012.12.036},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Dimensionality reduction,Divergence,Manifold learning,Stochastic neighbor embedding},
pages = {92--108},
title = {{Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in dimensionality reduction based on similarity preservation}},
volume = {112},
year = {2013}
}
@article{hartigan1979,
abstract = {The K-means clustering algorithm is described indetail by Hartigan(1975). An efficient version of the algorithm is presented here.$\backslash$nThe aim of the K-means algorithm is to divide M points in N dimensions into K clusters so that the within-cluster sum of squares is minimized. It is not practical to require that the solution has minimal sum of squares against all partitions except when M,N are small and K = 2. We seek instead "local" optima, solution such that no movement of a point from one cluster to another will reduce the within cluster sum of squares.},
author = {Hartigan, J. A. and Wong, M. A.},
doi = {10.2307/2346830},
isbn = {00359254},
issn = {00359254},
journal = {Applied Statistics},
keywords = {k-means clustering algorithm,transfer algorithm},
number = {1},
pages = {100--108},
pmid = {8705250},
title = {{A K-Means Clustering Algorithm}},
url = {http://www.jstor.org/stable/2346830{\%}5Cnpapers2://publication/uuid/6437C437-7330-4249-BD7B-2642C9B2C6BE{\%}5Cnhttp://www.jstor.org/stable/10.2307/2346830?origin=crossref},
volume = {28},
year = {1979}
}
@inproceedings{ester1996,
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
archivePrefix = {arXiv},
arxivId = {10.1.1.71.1980},
author = {Ester, Martin and Kriegel, Hans P and Sander, Jorg and Xu, Xiaowei},
booktitle = {Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining},
doi = {10.1.1.71.1980},
eprint = {10.1.1.71.1980},
isbn = {1577350049},
issn = {09758887},
keywords = {Arbitrary Shape of Clusters,Clustering Algorithms,Efficiency on Large Spatial Databases,Handling Noise},
pages = {226--231},
pmid = {15003161},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
url = {https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf},
year = {1996}
}
@article{quadrianto2011,
abstract = {We address the problem of metric learning for multi-view data, namely the construction of embedding projections from data in dif- ferent representations into a shared feature space, such that the Euclidean distance in this space provides a meaningful within-view as well as between-view similarity. Our moti- vation stems from the problem of cross-media retrieval tasks, where the availability of a joint Euclidean distance function is a pre- requisite to allow fast, in particular hashing- based, nearest neighbor queries. We formulate an objective function that ex- presses the intuitive concept that matching samples are mapped closely together in the output space, whereas non-matching samples are pushed apart, no matter in which view they are available. The resulting optimiza- tion problem is not convex, but it can be decomposed explicitly into a convex and a concave part, thereby allowing efficient op- timization using the convex-concave proce- dure. Experiments on an image retrieval task show that nearest-neighbor based cross- view retrieval is indeed possible, and the pro- posed technique improves the retrieval accu- racy over baseline techniques.},
author = {Quadrianto, Novi and Lampert, Christoph H},
doi = {10.1109/CVPR.2013.49},
isbn = {978-1-4503-0619-5},
journal = {Icml},
pages = {425--432},
title = {{Learning multi-view neighborhood preserving projections}},
url = {http://www.icml-2011.org/papers/304{\_}icmlpaper.pdf},
year = {2011}
}
@article{candes2009,
abstract = {Research supported in part by ONR grants N00014-09-1-0469 and N00014-08-1-0749},
author = {Candes, E. J. and Li, X. and Ma, Y. and Wright, J. and {E. J. Candes} and {X. Li} and {Y. Ma}},
journal = {Preprint},
pages = {41},
title = {{Robust Principal Component Analysis?}},
year = {2009}
}
@article{lugetoor2003,
abstract = {A key challenge for machine learning is tackling the problem of mining richly structured data sets, where the objects are linked in some way due to either an explicit or implicit relationship that exists between the objects. Links among the objects demonstrate certain patterns, which can be helpful for many machine learning tasks and are usually hard to capture with traditional statistical models. Recently there has been a surge of interest in this area, fueled largely by interest in web and hypertext mining, but also by interest in mining social networks, bibliographic citation data, epidemiological data and other domains best described using a linked or graph structure. In this paper we propose a framework for modeling link distributions, a link-based model that supports discriminative models describing both the link distributions and the attributes of linked objects. We use a structured logistic regression model, capturing both content and links. We systematically evaluate several variants of our link-based model on a range of data sets including both web and citation collections. In all cases, the use of the link distribution improves classification accuracy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lu, Qing and Getoor, Lise},
doi = {10.1007/1-84628-284-5_7},
eprint = {arXiv:1011.1669v3},
isbn = {9781577351894},
issn = {1098-6596},
journal = {Proceedings of the 20th International Conference on Machine Learning},
keywords = {lugetoor2003},
mendeley-tags = {lugetoor2003},
pages = {8},
pmid = {25246403},
title = {{Link-based Classification}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-066.pdf{\%}5Cnhttp://www.umiacs.umd.edu/{~}getoor/Publications/icml03.pdf},
year = {2003}
}
@article{white2012,
abstract = {Subspace learning seeks a low dimensional representation of data that enables accurate reconstruction. However, in many applications, data is obtained from multiple sources rather than a single source (e.g. an object might be viewed by cameras at different angles, or a document might consist of text and images). The conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation. In this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality. For this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally. Experiments illustrate that the proposed method produces high quality results.},
author = {White, Martha and Yu, Yaoliang and Zhang, Xinhua and Schuurmans, Dale},
isbn = {9781627480031},
issn = {10495258},
journal = {Proc.NIPS},
pages = {1673--1681},
title = {{Convex Multi-view Subspace Learning}},
year = {2012}
}
@article{chiaretti2004,
abstract = {Gene expression profiles were examined in 33 adult patients with T-cell acute lymphocytic leukemia (T-ALL). Nonspecific filtering criteria identified 313 genes differentially expressed in the leukemic cells. Hierarchical clustering of samples identified 2 groups that reflected the degree of T-cell differentiation but was not associated with clinical outcome. Comparison between refractory patients and those who responded to induction chemotherapy identified a single gene, interleukin 8 (IL-8), that was highly expressed in refractory T-ALL cells and a set of 30 genes that was highly expressed in leukemic cells from patients who achieved complete remission. We next identified 19 genes that were differentially expressed in T-ALL cells from patients who either had a relapse or remained in continuous complete remission. A model based on the expression of 3 of these genes was predictive of duration of remission. The 3-gene model was validated on a further set of T-ALL samples from 18 additional patients treated on the same clinical protocol. This study demonstrates that gene expression profiling can identify a limited number of genes that are predictive of response to induction therapy and remission duration in adult patients with T-ALL.},
author = {Chiaretti, Sabina and Li, Xiaochun and Gentleman, Robert and Vitale, Antonella and Vignetti, Marco and Mandelli, Franco and Ritz, Jerome and Foa, Robin},
doi = {10.1182/blood-2003-09-3243},
isbn = {10.1182/blood-2003-09-3243},
issn = {00064971},
journal = {Blood},
number = {7},
pages = {2771--2778},
pmid = {14684422},
title = {{Gene expression profile of adult T-cell acute lymphocytic leukemia identifies distinct subsets of patients with different response to therapy and survival}},
volume = {103},
year = {2004}
}
@article{nie2010,
abstract = {Feature selection is an important component of many machine learning applica- tions. Especially in many bioinformatics tasks, efficient and robust feature se- lection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with em- phasizing joint ?2,1-norm minimization on both loss function and regularization. The ?2,1-norm based loss function is robust to outliers in data points and the ?2,1- norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empir- ical studies are performed on six data sets to demonstrate the performance of our feature selection method.},
author = {Nie, Feiping and Huang, Heng and Cai, Xiao and Ding, Chris},
journal = {Advances in Neural Information Processing Systems},
pages = {1813--1821},
title = {{Efficient and Robust Feature Selection via Joint}},
volume = {23},
year = {2010}
}
@article{belkin2003,
abstract = {One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Belkin, Mikhail and Niyogi, Partha},
doi = {10.1162/089976603321780317},
eprint = {arXiv:1011.1669v3},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {6},
pages = {1373--1396},
pmid = {12816577},
title = {{Laplacian Eigenmaps for Dimensionality Reduction and Data Representation}},
volume = {15},
year = {2003}
}
@article{mitchell1998,
abstract = {We consider the problem of using a large unla- beled sample to boost performance of a learn- ing algorit,hrn when only a small set of labeled examples is available. In particular, we con- sider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the de- scription of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled ex- amples. Specifically, the presence of two dis- tinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algo- rithm's predictions on new unlabeled exam- ples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and un- labeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mitchell, Tom and Blum, Avrim},
doi = {10.1145/279943.279962},
eprint = {arXiv:1011.1669v3},
isbn = {1581130570},
issn = {1098-6596},
journal = {Proceedings of the eleventh annual conference on Computational learning theory},
pages = {92--100},
pmid = {25246403},
title = {{Combining labeled and unlabeled data with co-training}},
url = {http://dl.acm.org/citation.cfm?id=279943.279962},
year = {1998}
}
@article{desa1998,
abstract = {Humans and other animals learn to form complex categories without receiving a target output, or teaching signal, with each input pattern. In contrast, most computer algorithms that emulate such performance assume the brain is provided with the correct output at the neuronal level or require grossly unphysiological methods of information propagation. Natural environments do not contain explicit labeling signals, but they do contain important information in the form of temporal correlations between sensations to different sensory modalities, and humans are affected by this correlational structure (Howells, 1944; McGurk {\&} MacDonald, 1976; MacDonald {\&} McGurk, 1978; Zellner {\&} Kautz, 1990; Durgin {\&} Proffitt, 1996). In this article we describe a simple, unsupervised neural network algorithm that also uses this natural structure. Using only the co-occurring patterns of lip motion and sound signals from a human speaker, the network learns separate visual and auditory speech classifiers that perform comparably to supervised networks.},
author = {de Sa, Virginia R. and Ballard, Dana H.},
doi = {10.1162/089976698300017368},
isbn = {0899-7667 (Print)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural Computation},
number = {5},
pages = {1097--1117},
pmid = {9654768},
title = {{Category Learning Through Multimodality Sensing}},
url = {http://dx.doi.org/10.1162/089976698300017368{\%}5Cnhttp://www.mitpressjournals.org.proxy.uchicago.edu/doi/abs/10.1162/089976698300017368{\#}.VAvCAPldWSp{\%}5Cnhttp://www.mitpressjournals.org.proxy.uchicago.edu/doi/pdf/10.1162/089976698300017368{\#}.VAvCAPldWSp},
volume = {10},
year = {1998}
}
@article{Joachims2001,
abstract = {This paper develops a theoretical learning model of text classification for Support Vector Machines (SVMs). It connects the statistical properties of text-classification tasks with the generalization performance of a SVM in a quantitative way. Unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when SVMs perform well for text classification. In particular, it addresses the following questions: Why can support vector machines handle the large feature spaces in text classification effectively? How is this related to the statistical properties of text? What are sufficient conditions for applying SVMs to text-classification problems successfully?},
author = {Joachims, Thorsten},
doi = {10.1145/383952.383974},
isbn = {1-58113-331-6},
journal = {Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval},
pages = {128--136},
title = {{A statistical learning learning model of text classification for support vector machines}},
url = {http://portal.acm.org/citation.cfm?id=383952.383974},
year = {2001}
}
@inproceedings{Lampert2009,
abstract = {We study the problem of object classification when train-ing and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thou-sands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the cur-rent task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facil-itate research in this area, we have assembled a new large-scale dataset, " Animals with Attributes " , of over 30,000 an-imal images that match the 50 classes in Osherson's clas-sic table of how strongly humans associate 85 semantic at-tributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes.},
author = {Lampert, Christoph H and Nickisch, Hannes and Harmeling, Stefan},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
isbn = {9781424439911},
issn = {1063-6919},
pages = {951--958},
title = {{Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer}},
year = {2009}
}
@article{Amini2009,
abstract = {We address the problem of learning classifiers when observations$\backslash$r$\backslash$n  have multiple views, some of which may not be observed for all$\backslash$r$\backslash$n  examples.  We assume the existence of view generating functions$\backslash$r$\backslash$n  which may complete the missing views in an approximate way.  This$\backslash$r$\backslash$n  situation corresponds for example to learning text classifiers from$\backslash$r$\backslash$n  multilingual collections where documents are not available in all$\backslash$r$\backslash$n  languages.  In that case, Machine Translation (MT) systems may be$\backslash$r$\backslash$n  used to translate each document in the missing languages.  We derive$\backslash$r$\backslash$n  a generalization error bound for classifiers learned on examples$\backslash$r$\backslash$n  with multiple artificially created views. Our result uncovers a$\backslash$r$\backslash$n  trade-off between the size of the training set, the number of views,$\backslash$r$\backslash$n  and the quality of the view generating functions. As a consequence,$\backslash$r$\backslash$n  we identify situations where it is more interesting to use multiple$\backslash$r$\backslash$n  views for learning instead of classical single view learning.  An$\backslash$r$\backslash$n  extension of this framework is a natural way to leverage unlabeled$\backslash$r$\backslash$n  multi-view data in semi-supervised learning.  Experimental results$\backslash$r$\backslash$n  on a subset of the Reuters RCV1/RCV2 collections support our$\backslash$r$\backslash$n  findings by showing that additional views obtained from MT may$\backslash$r$\backslash$n  significantly improve the classification performance in the cases$\backslash$r$\backslash$n  identified by our trade-off.},
author = {Amini, Massih and Usunier, Nicolas and Goutte, Cyril},
isbn = {9781615679119},
issn = {1615679111},
journal = {Advances in Neural Information Processing Systems 22},
keywords = {Theory {\&} Algorithms},
pages = {28--36},
title = {{Learning from Multiple Partially Observed Views -- an Application to Multilingual Text Categorization}},
url = {http://eprints.pascal-network.org/archive/00006429/},
year = {2009}
}
@article{manning08,
abstract = {Introduction to Information Retrieval is the first textbook with a coherent treatment of classical and web information retrieval, including web search and the related areas of text classification and text clustering. Written from a computer science perspective, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. Designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also interest researchers and professionals. A complete set of lecture slides and exercises that accompany the book are available on the web.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
doi = {10.1002/asi.21234},
eprint = {0521865719 9780521865715},
isbn = {0521865719},
issn = {15322890},
journal = {Journal of the American Society for Information Science and Technology},
pages = {496},
pmid = {12049181},
title = {{Introduction to Information Retrieval}},
url = {http://nlp.stanford.edu/IR-book/information-retrieval-book.html},
volume = {1},
year = {2008}
}
@article{Stewart1991,
abstract = {This book is a comprehensive survey of matrix perturbation theory, a topic of interest to numerical analysts, statisticians, physical scientists, and engineers. In particular, the authors cover perturbation theory of linear systems and least square problems, the eignevalue problem, and the generalized eignevalue problem as wellas a complete treatment of vector and matrix norms, including the theory of unitary invariant norms.},
author = {Stewart, G W and Jovanovich, Harcourt Bruce},
doi = {10.1016/0378-4754(91)90038-5},
isbn = {9781466507289},
issn = {03784754},
journal = {Mathematics and Computers in Simulation},
number = {1},
pages = {74},
pmid = {21811041},
title = {{Matrix perturbation theory}},
volume = {33},
year = {1991}
}
@inproceedings{Cai2013,
abstract = {In past decade, more and more data are collected from multiple sources or represented by multiple views, where different views describe distinct per- spectives of the data. Although each view could be individually used for finding patterns by clustering, the clustering performance could be more accurate by exploring the rich information among multiple views. Several multi-view clustering methods have been proposed to unsupervised integrate different views of data. However, they are graph based ap- proaches, e.g. based on spectral clustering, such that they cannot handle the large-scale data. Howto combine these heterogeneous features for unsuper- vised large-scale data clustering has become a chal- lenging problem. In this paper, we propose a new robust large-scale multi-view clustering method to integrate heterogeneous representations of large- scale data. We evaluate the proposed new methods by six benchmark data sets and compared the per- formance with several commonly used clustering approaches as well as the baseline multi-view clus- tering methods. In all experimental results, our pro- posed methods consistently achieve superiors clus- tering performances.},
author = {Cai, Xiao and Nie, Feiping and Huang, Heng},
booktitle = {The 23rd International Joint Conference on Artificial Intelligence},
isbn = {9781577356332},
issn = {10450823},
keywords = {Web and Knowledge-Based Information Systems},
pages = {2598--2604},
title = {{Multi-View K -Means Clustering on Big Data}},
year = {2013}
}
@article{Li2015,
abstract = {In this paper, we address the problem of large-scale multi-view spectral clustering. In many real-world ap-plications, data can be represented in various heteroge-neous features or views. Different views often provide different aspects of information that are complementary to each other. Several previous methods of clustering have demonstrated that better accuracy can be achieved using integrated information of all the views than just using each view individually. One important class of such methods is multi-view spectral clustering, which is based on graph Laplacian. However, existing meth-ods are not applicable to large-scale problem for their high computational complexity. To this end, we pro-pose a novel large-scale multi-view spectral clustering approach based on the bipartite graph. Our method uses local manifold fusion to integrate heterogeneous fea-tures. To improve efficiency, we approximate the sim-ilarity graphs using bipartite graphs. Furthermore, we show that our method can be easily extended to handle the out-of-sample problem. Extensive experimental re-sults on five benchmark datasets demonstrate the effec-tiveness and efficiency of the proposed method, where our method runs up to nearly 3000 times faster than the state-of-the-art methods.},
author = {Li, Yeqing and Nie, Feiping and Huang, Heng and Huang, Junzhou},
file = {:home/samir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Large-Scale Multi-View Spectral Clustering with Bipartite Graph.pdf:pdf},
isbn = {9781577357025},
journal = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
keywords = {Novel Machine Learning Algorithms Track},
pages = {2750--2756},
title = {{Large-Scale Multi-View Spectral Clustering with Bipartite Graph}},
year = {2011}
}
@article{wang2013,
abstract = {Combining information from various data sources has become an important research topic in machine learning with many scien- tific applications. Most previous studies em- ploy kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of fea- tures in one source to an individual clus- ter of data can be varied, which makes the previous approaches ineffective. In this pa- per, we propose a novel multi-view learn- ing model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint struc- tured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoret- ical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods.},
author = {Wang, Hua and Nie, Feiping and Huang, Heng},
journal = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
pages = {352--360},
title = {{Multi-view clustering and feature learning via structured sparsity}},
volume = {28},
year = {2013}
}
@article{Zhao2014,
annote = {Supervised and Unsupervised Classification Techniques and their Applications},
author = {Zhao, Xuran and Evans, Nicholas and Dugelay, Jean-Luc},
issn = {0167-8655},
journal = {Pattern Recognition Letters},
keywords = {Multi-view clustering},
number = {0},
pages = {73--82},
title = {{A subspace co-training framework for multi-view clustering}},
volume = {41},
year = {2014}
}
@article{VEGA-PONS2011,
author = {Vega-Pons, Sandro and Ruiz-Shulcloper, Jos{\'{e}}},
doi = {10.1142/S0218001411008683},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
number = {03},
pages = {337--372},
title = {{a Survey of Clustering Ensemble Algorithms}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001411008683},
volume = {25},
year = {2011}
}
@article{Kumar2011,
abstract = {We propose a spectral clustering algorithm for the multi-view setting where we have ac- cess to multiple views of the data, each of which can be independently used for cluster- ing. Our spectral clustering algorithm has a flavor of co-training, which is already a widely used idea in semi-supervised learning. We work on the assumption that the true under- lying clustering would assign a point to the same cluster irrespective of the view. Hence, we constrain our approach to only search for the clusterings that agree across the views. Our algorithm does not have any hyperpa- rameters to set, which is a major advantage in unsupervised learning. We empirically compare with a number of baseline methods on synthetic and real-world datasets to show the efficacy of the proposed algorithm.},
author = {Kumar, Abhishek and Daum{\'{e}}, Hal},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
pages = {393--400},
title = {{A co-training approach for multi-view spectral clustering}},
year = {2011}
}
@article{Trendafilov2010,
abstract = {The standard common principal components (CPCs) may not always be useful for simultaneous dimensionality reduction in k groups. Moreover, the original FG algorithm finds the CPCs in arbitrary order, which does not reflect their importance with respect to the explained variance. A possible alternative is to find an approximate common subspace for all k groups. A new stepwise estimation procedure for obtaining CPCs is proposed, which imitates standard PCA. The stepwise CPCs facilitate simultaneous dimensionality reduction, as their variances are decreasing at least approximately in all k groups. Thus, they can be a better alternative for dimensionality reduction than the standard CPCs. The stepwise CPCs are found sequentially by a very simple algorithm, based on the well-known power method for a single covariance/correlation matrix. Numerical illustrations on well-known data are considered. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Trendafilov, Nickolay T.},
doi = {10.1016/j.csda.2010.03.010},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Definite matrices,Dimensionality reduction,Power iterations for k symmetric positive,Simultaneous diagonalization},
number = {12},
pages = {3446--3457},
title = {{Stepwise estimation of common principal components}},
volume = {54},
year = {2010}
}
@inproceedings{Chaudhuri2009,
abstract = {The apparent contrast of a centrally viewed Gabor target patch was measured by contrast matching in the presence of Gabor flanking patches positioned on a ring of radius r from the center of the target patch. Central patch apparent contrast was determined as a function of the number of flanking patches, the radius r of the ring, and the contrasts of both central and flanking patches. The apparent contrast of the central patch was reduced by the presence of the flanking patches for all experimental conditions. A two layer non-linear model for contrast perception accounts quite well for the data. The first layer performs a power function transformation on the contrast signals from the patches. The second layer takes the outputs from the first layer and divides them by one plus the square root of spatially weighted responses of nearby first layer mechanisms.},
address = {New York, NY, USA},
author = {Chaudhuri, Kamalika and Kakade, Sham M. and Livescu, Karen and Sridharan, Karthik},
booktitle = {Icml},
doi = {10.1145/1553374.1553391},
isbn = {9781605585161},
issn = {1605585165},
keywords = {canonical correlation analysis,clustering,edu,kamalika,multi-view learning,soe,ucsd},
pages = {1--8},
pmid = {8762716},
publisher = {ACM},
series = {ICML '09},
title = {{Multi-view clustering via canonical correlation analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553391},
year = {2009}
}
@article{maaten2008visualizing,
author = {van der Maaten, L J P and Hinton, G E},
journal = {Journal of Machine Learning Research},
keywords = {da mds projection visualization},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
year = {2008}
}
@article{Cui2007,
abstract = {Typical clustering algorithms output a single clustering of the data. However, in real world applications, data can often be interpreted in many different ways; data can have different groupings that are reasonable and interesting from different perspectives. This is especially true for high-dimensional data, where different feature subspaces may reveal different structures of the data. Why commit to one clustering solution while all these alternative clustering views might be interesting to the user. In this paper, we propose a new clustering paradigm for explorative data analysis: find all non-redundant clustering views of the data, where data points of one cluster can belong to different clusters in other views. We present a framework to solve this problem and suggest two approaches within this framework: (1) orthogonal clustering, and (2) clustering in orthogonal subspaces. In essence, both approaches find alternative ways to partition the data by projecting it to a space that is orthogonal to our current solution. The first approach seeks orthogonality in the cluster space, while the second approach seeks orthogonality in the feature space. We test our framework on both synthetic and high-dimensional benchmark data sets, and the results show that indeed our approaches were able to discover varied solutions that are interesting and meaningful.},
author = {Cui, Ying and Fern, Xiaoli Z. and Dy, Jennifer G.},
doi = {10.1109/ICDM.2007.94},
isbn = {0769530184},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Multi-view clustering,Non-redundant clustering,Orthogonalization},
month = {oct},
pages = {133--142},
title = {{Non-redundant multi-view clustering via orthogonalization}},
year = {2007}
}
@article{shimalik2000,
author = {Shi, Jianbo and Malik, Jitendra},
doi = {10.1109/CVPR.1997.609407},
issn = {1063-6919},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {computer vision,eigenvalues and eigenfunctions,gra},
number = {March},
pages = {888--905},
title = {{Normalized Cuts and Image Segmentation Normalized Cuts and Image Segmentation}},
volume = {22},
year = {2005}
}
@article{Ng01onspectral,
abstract = {Despite many empirical successes of spectral clustering methods algorithms that cluster points using eigenvectors of matrices derived from the datathere are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly dierent ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering.},
author = {Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
doi = {10.1.1.19.8100},
isbn = {1049-5258},
issn = {{\textless}null{\textgreater}},
journal = {Nips},
number = {14},
pages = {849--856},
pmid = {4078555},
publisher = {MIT Press},
title = {{On spectral clustering: Analysis and an algorithm}},
url = {http://www.nips.cc/NIPS2001/papers/psgz/AA35.ps.gz},
volume = {14},
year = {2001}
}
@article{Breukelen1998,
abstract = {Classifiers can be combined to reduce classification errors. We did experiments on a data set consisting of different sets of features of handwritten digits. Different types of classifiers were trained on these feature sets. The performances of these classifiers and combination rules were tested. The best results were acquired with the mean, median and product combination rules. The product was best for combining linear classifiers, the median for {\$}k{\$}-NN classifiers. Training a classifier on all features did not result in less errors.},
author = {Breukelen, M and Duin, R P W and Tax, D M J and Hartog, J E},
journal = {Kybernetika},
number = {4},
pages = {[381]--386},
publisher = {Institute of Information Theory and Automation AS CR},
title = {{Handwritten Digit Recognition by Combined Classifier (1998).pdf}},
volume = {34},
year = {1998}
}
@article{desa2005,
author = {de Sa, V.R. and Sa, Virginia R De and Diego, San and Jolla, La},
journal = {Development},
pages = {20--27},
title = {{Spectral Clustering with Two Views}},
url = {http://www.cogsci.ucsd.edu/academicPubs/desa/multiviewspectralclustering.pdf},
year = {1998}
}
@article{Flury2011,
abstract = {This article generalizes the method of principal components to so-called "common principal components" as follows: Consider the hypothesis that the covariance matrices $\Sigma$i for k populations are simultaneously diagonalizable. That is, there is an orthogonal matrix $\beta$ such that $\beta$'$\Sigma$i $\beta$ is diagonal for i = 1,..., k. I derive the normal-theory maximum likelihood estimates of the common component $\Sigma$i matrices and the log-likelihood-ratio statistics for testing this hypothesis. The solution has some favorable properties that do not depend on normality assumptions. Numerical examples illustrate the method. Applications to data reduction, multiple regression, and nonlinear discriminant analysis are sketched.},
author = {Flury, Bernhard N.},
doi = {10.1080/01621459.1984.10477108},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {and the simultaneously transformed,common principal com-,covariance mat- sines for,discriminant analysis,hc as the hypothesis,let us refer to,maximum likelihood,of common pc,p,rices,rotated axes,s,variables ui,xi will be called},
number = {388},
pages = {892--898},
title = {{Common Principal Components in K Groups}},
url = {http://www.jstor.org/stable/2288721{\%}5Cnhttp://www.jstor.org/stable/pdfplus/2288721.pdf},
volume = {79},
year = {1984}
}
@article{Kumar2011a,
abstract = {Abstract In many clustering problems, we have access to multiple views of the data each of which could be individually used for clustering. Exploiting information from multiple views, one can hope to find a clustering that is more accurate than the ones obtained using the ...},
author = {Kumar, Abhishek and Rai, Piyush and Daume, Hal},
doi = {10.1.1.229.2081},
isbn = {9781618395993},
journal = {Nips},
pages = {1413--1421},
title = {{Co-regularized Multi-view Spectral Clustering}},
url = {http://papers.nips.cc/paper/4360-co-regularized-multi-view-spectral-clustering},
year = {2011}
}
@inproceedings{Greene2009,
address = {Berlin, Heidelberg},
author = {Greene, Derek},
booktitle = {Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part I},
isbn = {978-3-642-04179-2},
pages = {423--438},
publisher = {Springer-Verlag},
series = {ECML PKDD '09},
title = {{A Matrix Factorization Approach for Integrating Multiple Data Views}},
year = {2009}
}
@article{Shao-YuanLI2014,
author = {{Shao-Yuan LI} and Jiang, Yuan and Zhou, Zhi-Hua},
file = {:home/samir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shao-Yuan LI, Jiang, Zhou - 2014 - Partial Multi-View Clustering.pdf:pdf},
journal = {Aaa},
keywords = {Novel Machine Learning Algorithms},
pages = {1968--1974},
title = {{Partial Multi-View Clustering}},
url = {http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/aaai14pvc.pdf},
year = {2014}
}
